srun: Job 4385042 step creation temporarily disabled, retrying
srun: Step created for job 4385042

Parameters:
	BATCH_SIZE=128
	CONCEPT_DIM=1
	CUDA=False
	DATASET=pathology
	DATASETS=['heart', 'ionosphere', 'breast-cancer', 'wine', 'heart', 'glass', 'diabetes', 'yeast', 'leukemia', 'abalone']
	DEBUG=False
	DROPOUT=0.1
	EMBEDDING=pathology
	EPOCHS=10
	H_SPARSITY=0.0001
	H_TYPE=cnn
	LIP_CALLS=10
	LIP_EPS=0.01
	LIP_POINTS=100
	LISA=1
	LOAD_MODEL=False
	LOG_PATH=log
	LR=0.001
	MODEL_PATH=models
	NCLASSES=2
	NCONCEPTS=15
	NOBIAS=False
	NUM_GPUS=1
	NUM_WORKERS=4
	OBJECTIVE=cross_entropy
	OPT=adam
	OPTIM=gp
	POSITIVE_THETA=False
	PRINT_FREQ=10
	RESULTS_PATH=out
	SEED=2018
	SUMMARY_PATH=results/summary.csv
	TEST=False
	THETA_ARCH=simple
	THETA_DIM=-1
	THETA_REG_LAMBDA=0.01
	THETA_REG_TYPE=grad3
	TRAIN=True
	WEIGHT_DECAY=0.001
Model path out lisa_output/models/mnist/grad3_Hcnn_Thsimple_Cpts15_Reg1e-02_Sp0.0001_LR0.001
Epoch: [0][0/422]  Time 2.73 (2.73)  Loss 2.3692 (2.3692)  Prec@1 7.031 (7.031)  Prec@5 50.000 (50.000)
Epoch: [0][10/422]  Time 2.25 (2.34)  Loss 2.0992 (2.2403)  Prec@1 34.375 (23.366)  Prec@5 81.250 (69.602)
Epoch: [0][20/422]  Time 2.23 (2.29)  Loss 1.6786 (2.0705)  Prec@1 46.875 (32.292)  Prec@5 93.750 (79.055)
Epoch: [0][30/422]  Time 2.25 (2.28)  Loss 1.2489 (1.9016)  Prec@1 70.312 (39.264)  Prec@5 96.875 (83.846)
Epoch: [0][40/422]  Time 2.22 (2.27)  Loss 1.1557 (1.7381)  Prec@1 68.750 (46.246)  Prec@5 95.312 (86.852)
Epoch: [0][50/422]  Time 2.24 (2.26)  Loss 0.8823 (1.6050)  Prec@1 84.375 (51.808)  Prec@5 100.000 (88.940)
Epoch: [0][60/422]  Time 2.21 (2.26)  Loss 0.8434 (1.4868)  Prec@1 86.719 (56.673)  Prec@5 98.438 (90.471)
Epoch: [0][70/422]  Time 2.25 (2.26)  Loss 0.7239 (1.3929)  Prec@1 85.938 (60.497)  Prec@5 98.438 (91.560)
Epoch: [0][80/422]  Time 2.23 (2.26)  Loss 0.8014 (1.3210)  Prec@1 82.812 (63.522)  Prec@5 98.438 (92.371)
Epoch: [0][90/422]  Time 2.25 (2.26)  Loss 0.7187 (1.2599)  Prec@1 85.938 (65.831)  Prec@5 99.219 (93.098)
Epoch: [0][100/422]  Time 2.25 (2.26)  Loss 0.6594 (1.2059)  Prec@1 90.625 (67.830)  Prec@5 99.219 (93.665)
Epoch: [0][110/422]  Time 2.25 (2.26)  Loss 0.6501 (1.1563)  Prec@1 91.406 (69.609)  Prec@5 99.219 (94.179)
Epoch: [0][120/422]  Time 2.24 (2.26)  Loss 0.6029 (1.1131)  Prec@1 92.969 (71.294)  Prec@5 100.000 (94.609)
Epoch: [0][130/422]  Time 2.25 (2.25)  Loss 0.5731 (1.0761)  Prec@1 90.625 (72.644)  Prec@5 100.000 (94.949)
Epoch: [0][140/422]  Time 2.23 (2.25)  Loss 0.6668 (1.0461)  Prec@1 85.938 (73.814)  Prec@5 100.000 (95.218)
Epoch: [0][150/422]  Time 2.23 (2.25)  Loss 0.4998 (1.0158)  Prec@1 92.969 (74.912)  Prec@5 100.000 (95.488)
Epoch: [0][160/422]  Time 2.23 (2.25)  Loss 0.5742 (0.9889)  Prec@1 88.281 (75.903)  Prec@5 100.000 (95.720)
Epoch: [0][170/422]  Time 2.23 (2.25)  Loss 0.6378 (0.9657)  Prec@1 89.844 (76.732)  Prec@5 96.875 (95.911)
Epoch: [0][180/422]  Time 2.28 (2.25)  Loss 0.5623 (0.9442)  Prec@1 89.844 (77.525)  Prec@5 99.219 (96.072)
Epoch: [0][190/422]  Time 2.25 (2.25)  Loss 0.4886 (0.9249)  Prec@1 93.750 (78.195)  Prec@5 100.000 (96.249)
Epoch: [0][200/422]  Time 2.23 (2.25)  Loss 0.5915 (0.9065)  Prec@1 85.938 (78.840)  Prec@5 99.219 (96.409)
Epoch: [0][210/422]  Time 2.26 (2.25)  Loss 0.5686 (0.8914)  Prec@1 89.062 (79.376)  Prec@5 99.219 (96.534)
Epoch: [0][220/422]  Time 2.24 (2.25)  Loss 0.5095 (0.8753)  Prec@1 93.750 (79.942)  Prec@5 99.219 (96.652)
Epoch: [0][230/422]  Time 2.24 (2.25)  Loss 0.5082 (0.8597)  Prec@1 92.188 (80.482)  Prec@5 100.000 (96.774)
Epoch: [0][240/422]  Time 2.22 (2.25)  Loss 0.5272 (0.8456)  Prec@1 89.844 (80.958)  Prec@5 99.219 (96.869)
Epoch: [0][250/422]  Time 2.25 (2.25)  Loss 0.5395 (0.8303)  Prec@1 91.406 (81.474)  Prec@5 99.219 (96.978)
Epoch: [0][260/422]  Time 2.25 (2.25)  Loss 0.5183 (0.8169)  Prec@1 93.750 (81.953)  Prec@5 98.438 (97.070)
Epoch: [0][270/422]  Time 2.23 (2.25)  Loss 0.5030 (0.8038)  Prec@1 91.406 (82.394)  Prec@5 100.000 (97.158)
Epoch: [0][280/422]  Time 2.22 (2.25)  Loss 0.4161 (0.7917)  Prec@1 93.750 (82.765)  Prec@5 100.000 (97.248)
Epoch: [0][290/422]  Time 2.25 (2.25)  Loss 0.5143 (0.7817)  Prec@1 90.625 (83.076)  Prec@5 98.438 (97.318)
Epoch: [0][300/422]  Time 2.23 (2.25)  Loss 0.4650 (0.7707)  Prec@1 95.312 (83.433)  Prec@5 98.438 (97.371)
Epoch: [0][310/422]  Time 2.24 (2.25)  Loss 0.4763 (0.7620)  Prec@1 89.844 (83.702)  Prec@5 99.219 (97.433)
Epoch: [0][320/422]  Time 2.23 (2.25)  Loss 0.4411 (0.7528)  Prec@1 94.531 (83.986)  Prec@5 100.000 (97.498)
Epoch: [0][330/422]  Time 2.24 (2.25)  Loss 0.4647 (0.7441)  Prec@1 95.312 (84.285)  Prec@5 99.219 (97.559)
Epoch: [0][340/422]  Time 2.23 (2.25)  Loss 0.4712 (0.7352)  Prec@1 92.188 (84.574)  Prec@5 100.000 (97.620)
Epoch: [0][350/422]  Time 2.26 (2.25)  Loss 0.5003 (0.7277)  Prec@1 92.188 (84.829)  Prec@5 98.438 (97.658)
Epoch: [0][360/422]  Time 2.24 (2.25)  Loss 0.4316 (0.7197)  Prec@1 92.969 (85.074)  Prec@5 100.000 (97.719)
Epoch: [0][370/422]  Time 2.24 (2.25)  Loss 0.4885 (0.7129)  Prec@1 94.531 (85.293)  Prec@5 98.438 (97.762)
Epoch: [0][380/422]  Time 2.23 (2.25)  Loss 0.4805 (0.7059)  Prec@1 92.188 (85.529)  Prec@5 99.219 (97.802)
Epoch: [0][390/422]  Time 2.24 (2.25)  Loss 0.4697 (0.6997)  Prec@1 92.188 (85.726)  Prec@5 100.000 (97.844)
Epoch: [0][400/422]  Time 2.22 (2.25)  Loss 0.4463 (0.6934)  Prec@1 93.750 (85.918)  Prec@5 99.219 (97.884)
Epoch: [0][410/422]  Time 2.20 (2.25)  Loss 0.5477 (0.6882)  Prec@1 89.062 (86.061)  Prec@5 99.219 (97.920)
Epoch: [0][420/422]  Time 2.22 (2.25)  Loss 0.5167 (0.6821)  Prec@1 89.844 (86.259)  Prec@5 99.219 (97.959)
Test: [0/47]	Time 0.380 (0.380)	Loss 0.1937 (0.1937)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.092 (0.126)	Loss 0.2438 (0.1582)	Prec@1 95.312 (96.094)	Prec@5 100.000 (99.645)
Test: [20/47]	Time 0.094 (0.111)	Loss 0.1552 (0.1273)	Prec@1 94.531 (96.503)	Prec@5 100.000 (99.702)
Test: [30/47]	Time 0.093 (0.105)	Loss 0.0704 (0.1251)	Prec@1 96.875 (96.573)	Prec@5 100.000 (99.698)
Test: [40/47]	Time 0.072 (0.102)	Loss 0.0946 (0.1217)	Prec@1 97.656 (96.761)	Prec@5 100.000 (99.771)
 * Prec@1 96.650 Prec@5 99.783
Epoch: [1][0/422]  Time 2.73 (2.73)  Loss 0.4980 (0.4980)  Prec@1 91.406 (91.406)  Prec@5 99.219 (99.219)
Epoch: [1][10/422]  Time 2.23 (2.38)  Loss 0.3754 (0.4138)  Prec@1 96.875 (94.673)  Prec@5 100.000 (99.574)
Epoch: [1][20/422]  Time 2.22 (2.32)  Loss 0.3920 (0.4269)  Prec@1 94.531 (94.085)  Prec@5 99.219 (99.516)
Epoch: [1][30/422]  Time 2.23 (2.29)  Loss 0.4001 (0.4264)  Prec@1 96.094 (94.229)  Prec@5 100.000 (99.546)
Epoch: [1][40/422]  Time 2.22 (2.28)  Loss 0.4723 (0.4366)  Prec@1 92.188 (93.960)  Prec@5 99.219 (99.486)
Epoch: [1][50/422]  Time 2.24 (2.27)  Loss 0.4337 (0.4319)  Prec@1 92.188 (93.949)  Prec@5 100.000 (99.479)
Epoch: [1][60/422]  Time 2.22 (2.26)  Loss 0.3955 (0.4268)  Prec@1 94.531 (94.057)  Prec@5 100.000 (99.526)
Epoch: [1][70/422]  Time 2.22 (2.26)  Loss 0.4589 (0.4286)  Prec@1 92.969 (93.882)  Prec@5 99.219 (99.527)
Epoch: [1][80/422]  Time 2.23 (2.26)  Loss 0.2943 (0.4296)  Prec@1 96.875 (93.818)  Prec@5 100.000 (99.527)
Epoch: [1][90/422]  Time 2.23 (2.25)  Loss 0.4476 (0.4264)  Prec@1 92.969 (93.896)  Prec@5 100.000 (99.554)
Epoch: [1][100/422]  Time 2.24 (2.25)  Loss 0.3913 (0.4235)  Prec@1 92.188 (93.920)  Prec@5 100.000 (99.582)
Epoch: [1][110/422]  Time 2.20 (2.25)  Loss 0.3125 (0.4213)  Prec@1 98.438 (93.989)  Prec@5 99.219 (99.578)
Epoch: [1][120/422]  Time 2.23 (2.25)  Loss 0.3802 (0.4189)  Prec@1 96.875 (94.073)  Prec@5 100.000 (99.600)
Epoch: [1][130/422]  Time 2.21 (2.25)  Loss 0.3653 (0.4173)  Prec@1 95.312 (94.108)  Prec@5 100.000 (99.606)
Epoch: [1][140/422]  Time 2.23 (2.25)  Loss 0.3920 (0.4163)  Prec@1 95.312 (94.160)  Prec@5 100.000 (99.596)
Epoch: [1][150/422]  Time 2.21 (2.24)  Loss 0.5410 (0.4161)  Prec@1 90.625 (94.143)  Prec@5 97.656 (99.576)
Epoch: [1][160/422]  Time 2.42 (2.25)  Loss 0.3921 (0.4154)  Prec@1 95.312 (94.206)  Prec@5 100.000 (99.563)
Epoch: [1][170/422]  Time 2.40 (2.26)  Loss 0.3486 (0.4156)  Prec@1 94.531 (94.193)  Prec@5 100.000 (99.539)
Epoch: [1][180/422]  Time 2.40 (2.27)  Loss 0.2988 (0.4142)  Prec@1 98.438 (94.216)  Prec@5 100.000 (99.542)
Epoch: [1][190/422]  Time 2.40 (2.27)  Loss 0.3833 (0.4140)  Prec@1 94.531 (94.208)  Prec@5 100.000 (99.538)
Epoch: [1][200/422]  Time 2.30 (2.28)  Loss 0.4310 (0.4121)  Prec@1 94.531 (94.259)  Prec@5 99.219 (99.553)
Epoch: [1][210/422]  Time 2.23 (2.27)  Loss 0.3778 (0.4100)  Prec@1 96.094 (94.298)  Prec@5 100.000 (99.556)
Epoch: [1][220/422]  Time 2.22 (2.27)  Loss 0.4528 (0.4094)  Prec@1 92.188 (94.301)  Prec@5 97.656 (99.555)
Epoch: [1][230/422]  Time 2.23 (2.27)  Loss 0.3566 (0.4078)  Prec@1 96.094 (94.345)  Prec@5 100.000 (99.570)
Epoch: [1][240/422]  Time 2.33 (2.27)  Loss 0.3470 (0.4076)  Prec@1 95.312 (94.317)  Prec@5 100.000 (99.575)
Epoch: [1][250/422]  Time 2.21 (2.27)  Loss 0.3183 (0.4060)  Prec@1 97.656 (94.323)  Prec@5 100.000 (99.577)
Epoch: [1][260/422]  Time 2.24 (2.27)  Loss 0.3492 (0.4051)  Prec@1 96.094 (94.337)  Prec@5 100.000 (99.572)
Epoch: [1][270/422]  Time 2.23 (2.27)  Loss 0.3824 (0.4048)  Prec@1 96.094 (94.350)  Prec@5 99.219 (99.579)
Epoch: [1][280/422]  Time 2.32 (2.26)  Loss 0.3385 (0.4038)  Prec@1 96.094 (94.384)  Prec@5 100.000 (99.586)
Epoch: [1][290/422]  Time 2.23 (2.26)  Loss 0.4097 (0.4027)  Prec@1 93.750 (94.421)  Prec@5 98.438 (99.584)
Epoch: [1][300/422]  Time 2.22 (2.26)  Loss 0.4074 (0.4023)  Prec@1 92.969 (94.414)  Prec@5 100.000 (99.593)
Epoch: [1][310/422]  Time 2.19 (2.26)  Loss 0.3596 (0.4018)  Prec@1 94.531 (94.416)  Prec@5 98.438 (99.596)
Epoch: [1][320/422]  Time 2.23 (2.26)  Loss 0.5276 (0.4009)  Prec@1 88.281 (94.429)  Prec@5 98.438 (99.591)
Epoch: [1][330/422]  Time 2.23 (2.26)  Loss 0.3350 (0.3997)  Prec@1 95.312 (94.460)  Prec@5 100.000 (99.599)
Epoch: [1][340/422]  Time 2.21 (2.26)  Loss 0.3193 (0.3983)  Prec@1 97.656 (94.515)  Prec@5 100.000 (99.604)
Epoch: [1][350/422]  Time 2.23 (2.26)  Loss 0.3249 (0.3976)  Prec@1 95.312 (94.525)  Prec@5 99.219 (99.606)
Epoch: [1][360/422]  Time 2.23 (2.26)  Loss 0.3050 (0.3973)  Prec@1 96.875 (94.527)  Prec@5 100.000 (99.602)
Epoch: [1][370/422]  Time 2.23 (2.25)  Loss 0.3239 (0.3962)  Prec@1 96.094 (94.557)  Prec@5 100.000 (99.602)
Epoch: [1][380/422]  Time 2.21 (2.25)  Loss 0.3155 (0.3959)  Prec@1 96.875 (94.564)  Prec@5 100.000 (99.602)
Epoch: [1][390/422]  Time 2.21 (2.25)  Loss 0.3454 (0.3955)  Prec@1 97.656 (94.571)  Prec@5 100.000 (99.604)
Epoch: [1][400/422]  Time 2.21 (2.25)  Loss 0.4473 (0.3952)  Prec@1 91.406 (94.564)  Prec@5 99.219 (99.597)
Epoch: [1][410/422]  Time 2.21 (2.25)  Loss 0.3298 (0.3941)  Prec@1 97.656 (94.600)  Prec@5 100.000 (99.603)
Epoch: [1][420/422]  Time 2.21 (2.25)  Loss 0.3473 (0.3926)  Prec@1 94.531 (94.639)  Prec@5 100.000 (99.599)
Test: [0/47]	Time 0.399 (0.399)	Loss 0.2673 (0.2673)	Prec@1 95.312 (95.312)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.093 (0.129)	Loss 0.0516 (0.1160)	Prec@1 97.656 (96.591)	Prec@5 100.000 (99.858)
Test: [20/47]	Time 0.093 (0.112)	Loss 0.1572 (0.1036)	Prec@1 94.531 (96.987)	Prec@5 100.000 (99.926)
Test: [30/47]	Time 0.094 (0.106)	Loss 0.0784 (0.1021)	Prec@1 98.438 (97.329)	Prec@5 100.000 (99.950)
Test: [40/47]	Time 0.070 (0.102)	Loss 0.0696 (0.1020)	Prec@1 98.438 (97.351)	Prec@5 100.000 (99.924)
 * Prec@1 97.433 Prec@5 99.933
Epoch: [2][0/422]  Time 2.69 (2.69)  Loss 0.3492 (0.3492)  Prec@1 93.750 (93.750)  Prec@5 100.000 (100.000)
Epoch: [2][10/422]  Time 2.23 (2.28)  Loss 0.3823 (0.3569)  Prec@1 94.531 (95.312)  Prec@5 100.000 (99.787)
Epoch: [2][20/422]  Time 2.21 (2.25)  Loss 0.3183 (0.3687)  Prec@1 96.875 (95.275)  Prec@5 99.219 (99.591)
Epoch: [2][30/422]  Time 2.22 (2.24)  Loss 0.3765 (0.3651)  Prec@1 94.531 (95.439)  Prec@5 99.219 (99.496)
Epoch: [2][40/422]  Time 2.22 (2.24)  Loss 0.3961 (0.3669)  Prec@1 93.750 (95.351)  Prec@5 99.219 (99.466)
Epoch: [2][50/422]  Time 2.21 (2.24)  Loss 0.3233 (0.3639)  Prec@1 96.094 (95.374)  Prec@5 100.000 (99.464)
Epoch: [2][60/422]  Time 2.35 (2.24)  Loss 0.2916 (0.3603)  Prec@1 97.656 (95.530)  Prec@5 100.000 (99.526)
Epoch: [2][70/422]  Time 2.22 (2.24)  Loss 0.3687 (0.3639)  Prec@1 93.750 (95.423)  Prec@5 100.000 (99.549)
Epoch: [2][80/422]  Time 2.20 (2.23)  Loss 0.3310 (0.3637)  Prec@1 96.094 (95.351)  Prec@5 100.000 (99.576)
Epoch: [2][90/422]  Time 2.20 (2.23)  Loss 0.3937 (0.3630)  Prec@1 94.531 (95.304)  Prec@5 100.000 (99.596)
Epoch: [2][100/422]  Time 2.20 (2.23)  Loss 0.3123 (0.3635)  Prec@1 96.094 (95.351)  Prec@5 100.000 (99.590)
Epoch: [2][110/422]  Time 2.24 (2.23)  Loss 0.3052 (0.3608)  Prec@1 96.875 (95.369)  Prec@5 100.000 (99.620)
Epoch: [2][120/422]  Time 2.21 (2.23)  Loss 0.3377 (0.3594)  Prec@1 96.094 (95.396)  Prec@5 100.000 (99.632)
Epoch: [2][130/422]  Time 2.23 (2.23)  Loss 0.3780 (0.3592)  Prec@1 95.312 (95.450)  Prec@5 100.000 (99.624)
Epoch: [2][140/422]  Time 2.21 (2.23)  Loss 0.4413 (0.3580)  Prec@1 93.750 (95.495)  Prec@5 96.875 (99.601)
Epoch: [2][150/422]  Time 2.23 (2.23)  Loss 0.4152 (0.3564)  Prec@1 92.969 (95.514)  Prec@5 100.000 (99.617)
Epoch: [2][160/422]  Time 2.21 (2.23)  Loss 0.4227 (0.3566)  Prec@1 92.188 (95.473)  Prec@5 99.219 (99.602)
Epoch: [2][170/422]  Time 2.22 (2.23)  Loss 0.3830 (0.3579)  Prec@1 92.188 (95.408)  Prec@5 99.219 (99.603)
Epoch: [2][180/422]  Time 2.20 (2.23)  Loss 0.3528 (0.3569)  Prec@1 95.312 (95.442)  Prec@5 100.000 (99.620)
Epoch: [2][190/422]  Time 2.21 (2.23)  Loss 0.3603 (0.3564)  Prec@1 95.312 (95.460)  Prec@5 99.219 (99.624)
Epoch: [2][200/422]  Time 2.22 (2.23)  Loss 0.3693 (0.3563)  Prec@1 94.531 (95.449)  Prec@5 99.219 (99.615)
Epoch: [2][210/422]  Time 2.22 (2.23)  Loss 0.3916 (0.3559)  Prec@1 93.750 (95.464)  Prec@5 100.000 (99.619)
Epoch: [2][220/422]  Time 2.34 (2.23)  Loss 0.2825 (0.3546)  Prec@1 98.438 (95.482)  Prec@5 100.000 (99.622)
Epoch: [2][230/422]  Time 2.40 (2.23)  Loss 0.3247 (0.3544)  Prec@1 95.312 (95.512)  Prec@5 100.000 (99.618)
Epoch: [2][240/422]  Time 2.43 (2.24)  Loss 0.3824 (0.3543)  Prec@1 92.188 (95.488)  Prec@5 100.000 (99.617)
Epoch: [2][250/422]  Time 2.41 (2.25)  Loss 0.3565 (0.3547)  Prec@1 94.531 (95.477)  Prec@5 100.000 (99.614)
Epoch: [2][260/422]  Time 2.23 (2.25)  Loss 0.2768 (0.3539)  Prec@1 98.438 (95.477)  Prec@5 100.000 (99.623)
Epoch: [2][270/422]  Time 2.22 (2.25)  Loss 0.3852 (0.3542)  Prec@1 94.531 (95.471)  Prec@5 100.000 (99.634)
Epoch: [2][280/422]  Time 2.21 (2.25)  Loss 0.2875 (0.3539)  Prec@1 98.438 (95.477)  Prec@5 100.000 (99.636)
Epoch: [2][290/422]  Time 2.21 (2.25)  Loss 0.3560 (0.3533)  Prec@1 94.531 (95.484)  Prec@5 100.000 (99.635)
Epoch: [2][300/422]  Time 2.23 (2.25)  Loss 0.2791 (0.3532)  Prec@1 98.438 (95.484)  Prec@5 100.000 (99.642)
Epoch: [2][310/422]  Time 2.19 (2.25)  Loss 0.3093 (0.3532)  Prec@1 96.875 (95.476)  Prec@5 100.000 (99.646)
Epoch: [2][320/422]  Time 2.21 (2.25)  Loss 0.3640 (0.3536)  Prec@1 96.875 (95.463)  Prec@5 100.000 (99.647)
Epoch: [2][330/422]  Time 2.23 (2.24)  Loss 0.3207 (0.3525)  Prec@1 94.531 (95.492)  Prec@5 100.000 (99.651)
Epoch: [2][340/422]  Time 2.22 (2.24)  Loss 0.3292 (0.3521)  Prec@1 95.312 (95.514)  Prec@5 100.000 (99.654)
Epoch: [2][350/422]  Time 2.21 (2.24)  Loss 0.2672 (0.3513)  Prec@1 99.219 (95.531)  Prec@5 100.000 (99.655)
Epoch: [2][360/422]  Time 2.22 (2.24)  Loss 0.3643 (0.3517)  Prec@1 94.531 (95.518)  Prec@5 100.000 (99.656)
Epoch: [2][370/422]  Time 2.21 (2.24)  Loss 0.3638 (0.3511)  Prec@1 92.969 (95.523)  Prec@5 100.000 (99.657)
Epoch: [2][380/422]  Time 2.22 (2.24)  Loss 0.2795 (0.3510)  Prec@1 97.656 (95.530)  Prec@5 100.000 (99.653)
Epoch: [2][390/422]  Time 2.21 (2.24)  Loss 0.4371 (0.3516)  Prec@1 94.531 (95.510)  Prec@5 99.219 (99.644)
Epoch: [2][400/422]  Time 2.22 (2.24)  Loss 0.2864 (0.3512)  Prec@1 97.656 (95.531)  Prec@5 100.000 (99.643)
Epoch: [2][410/422]  Time 2.23 (2.24)  Loss 0.4026 (0.3508)  Prec@1 92.969 (95.541)  Prec@5 99.219 (99.645)
Epoch: [2][420/422]  Time 2.20 (2.24)  Loss 0.3236 (0.3502)  Prec@1 96.875 (95.552)  Prec@5 99.219 (99.647)
Test: [0/47]	Time 0.318 (0.318)	Loss 0.0911 (0.0911)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.096 (0.123)	Loss 0.0840 (0.1075)	Prec@1 97.656 (97.585)	Prec@5 100.000 (99.929)
Test: [20/47]	Time 0.093 (0.109)	Loss 0.1199 (0.0995)	Prec@1 97.656 (97.545)	Prec@5 100.000 (99.963)
Test: [30/47]	Time 0.095 (0.105)	Loss 0.0212 (0.0899)	Prec@1 99.219 (97.707)	Prec@5 100.000 (99.975)
Test: [40/47]	Time 0.074 (0.101)	Loss 0.1974 (0.0929)	Prec@1 95.312 (97.599)	Prec@5 99.219 (99.962)
 * Prec@1 97.667 Prec@5 99.967
Epoch: [3][0/422]  Time 2.58 (2.58)  Loss 0.2841 (0.2841)  Prec@1 98.438 (98.438)  Prec@5 100.000 (100.000)
Epoch: [3][10/422]  Time 2.52 (2.46)  Loss 0.3420 (0.3361)  Prec@1 95.312 (96.094)  Prec@5 100.000 (99.574)
Epoch: [3][20/422]  Time 2.34 (2.40)  Loss 0.3038 (0.3346)  Prec@1 96.875 (95.796)  Prec@5 99.219 (99.591)
Epoch: [3][30/422]  Time 2.42 (2.39)  Loss 0.3476 (0.3355)  Prec@1 93.750 (95.665)  Prec@5 100.000 (99.572)
Epoch: [3][40/422]  Time 2.41 (2.39)  Loss 0.3232 (0.3285)  Prec@1 96.875 (95.770)  Prec@5 100.000 (99.619)
Epoch: [3][50/422]  Time 2.42 (2.40)  Loss 0.3953 (0.3326)  Prec@1 95.312 (95.634)  Prec@5 100.000 (99.617)
Epoch: [3][60/422]  Time 2.27 (2.40)  Loss 0.3141 (0.3347)  Prec@1 96.094 (95.633)  Prec@5 100.000 (99.641)
Epoch: [3][70/422]  Time 2.21 (2.37)  Loss 0.3327 (0.3352)  Prec@1 97.656 (95.687)  Prec@5 99.219 (99.648)
Epoch: [3][80/422]  Time 2.39 (2.37)  Loss 0.4235 (0.3373)  Prec@1 92.969 (95.689)  Prec@5 100.000 (99.662)
Epoch: [3][90/422]  Time 2.39 (2.37)  Loss 0.2955 (0.3345)  Prec@1 98.438 (95.750)  Prec@5 100.000 (99.682)
Epoch: [3][100/422]  Time 2.39 (2.38)  Loss 0.2768 (0.3353)  Prec@1 96.875 (95.769)  Prec@5 100.000 (99.691)
Epoch: [3][110/422]  Time 2.32 (2.38)  Loss 0.3294 (0.3370)  Prec@1 94.531 (95.714)  Prec@5 99.219 (99.697)
Epoch: [3][120/422]  Time 2.23 (2.37)  Loss 0.3312 (0.3353)  Prec@1 95.312 (95.745)  Prec@5 100.000 (99.709)
Epoch: [3][130/422]  Time 2.38 (2.37)  Loss 0.3110 (0.3334)  Prec@1 97.656 (95.825)  Prec@5 100.000 (99.714)
Epoch: [3][140/422]  Time 2.41 (2.37)  Loss 0.2675 (0.3332)  Prec@1 98.438 (95.822)  Prec@5 100.000 (99.723)
Epoch: [3][150/422]  Time 2.22 (2.36)  Loss 0.2438 (0.3324)  Prec@1 98.438 (95.866)  Prec@5 100.000 (99.731)
Epoch: [3][160/422]  Time 2.21 (2.35)  Loss 0.3792 (0.3335)  Prec@1 92.969 (95.793)  Prec@5 100.000 (99.733)
Epoch: [3][170/422]  Time 2.22 (2.35)  Loss 0.3719 (0.3336)  Prec@1 93.750 (95.779)  Prec@5 100.000 (99.730)
Epoch: [3][180/422]  Time 2.24 (2.34)  Loss 0.3728 (0.3315)  Prec@1 93.750 (95.826)  Prec@5 100.000 (99.737)
Epoch: [3][190/422]  Time 2.21 (2.34)  Loss 0.3620 (0.3309)  Prec@1 95.312 (95.848)  Prec@5 98.438 (99.722)
Epoch: [3][200/422]  Time 2.23 (2.33)  Loss 0.2988 (0.3310)  Prec@1 98.438 (95.853)  Prec@5 100.000 (99.716)
Epoch: [3][210/422]  Time 2.21 (2.32)  Loss 0.2407 (0.3299)  Prec@1 98.438 (95.894)  Prec@5 100.000 (99.726)
Epoch: [3][220/422]  Time 2.21 (2.32)  Loss 0.3510 (0.3291)  Prec@1 95.312 (95.899)  Prec@5 98.438 (99.728)
Epoch: [3][230/422]  Time 2.40 (2.32)  Loss 0.3294 (0.3280)  Prec@1 95.312 (95.921)  Prec@5 100.000 (99.733)
Epoch: [3][240/422]  Time 2.38 (2.33)  Loss 0.3355 (0.3283)  Prec@1 96.875 (95.909)  Prec@5 100.000 (99.734)
Epoch: [3][250/422]  Time 2.21 (2.33)  Loss 0.3846 (0.3280)  Prec@1 93.750 (95.913)  Prec@5 100.000 (99.732)
Epoch: [3][260/422]  Time 2.22 (2.32)  Loss 0.3583 (0.3285)  Prec@1 95.312 (95.890)  Prec@5 100.000 (99.725)
Epoch: [3][270/422]  Time 2.22 (2.32)  Loss 0.2704 (0.3280)  Prec@1 96.875 (95.903)  Prec@5 100.000 (99.726)
Epoch: [3][280/422]  Time 2.34 (2.32)  Loss 0.2700 (0.3275)  Prec@1 96.875 (95.919)  Prec@5 100.000 (99.711)
Epoch: [3][290/422]  Time 2.22 (2.31)  Loss 0.2468 (0.3271)  Prec@1 96.875 (95.917)  Prec@5 100.000 (99.710)
Epoch: [3][300/422]  Time 2.23 (2.31)  Loss 0.2826 (0.3264)  Prec@1 97.656 (95.933)  Prec@5 99.219 (99.704)
Epoch: [3][310/422]  Time 2.22 (2.31)  Loss 0.2800 (0.3266)  Prec@1 98.438 (95.933)  Prec@5 99.219 (99.694)
Epoch: [3][320/422]  Time 2.22 (2.31)  Loss 0.2583 (0.3259)  Prec@1 99.219 (95.950)  Prec@5 100.000 (99.698)
Epoch: [3][330/422]  Time 2.22 (2.30)  Loss 0.4370 (0.3259)  Prec@1 95.312 (95.962)  Prec@5 99.219 (99.698)
Epoch: [3][340/422]  Time 2.21 (2.30)  Loss 0.3116 (0.3256)  Prec@1 96.094 (95.963)  Prec@5 100.000 (99.698)
Epoch: [3][350/422]  Time 2.22 (2.30)  Loss 0.3759 (0.3258)  Prec@1 92.969 (95.956)  Prec@5 98.438 (99.695)
Epoch: [3][360/422]  Time 2.23 (2.30)  Loss 0.3030 (0.3255)  Prec@1 95.312 (95.966)  Prec@5 100.000 (99.691)
Epoch: [3][370/422]  Time 2.22 (2.29)  Loss 0.2944 (0.3251)  Prec@1 97.656 (95.982)  Prec@5 100.000 (99.697)
Epoch: [3][380/422]  Time 2.41 (2.30)  Loss 0.3361 (0.3253)  Prec@1 96.875 (95.977)  Prec@5 100.000 (99.699)
Epoch: [3][390/422]  Time 2.41 (2.30)  Loss 0.2692 (0.3257)  Prec@1 96.875 (95.978)  Prec@5 100.000 (99.702)
Epoch: [3][400/422]  Time 2.31 (2.30)  Loss 0.3108 (0.3253)  Prec@1 96.875 (95.989)  Prec@5 99.219 (99.702)
Epoch: [3][410/422]  Time 2.22 (2.30)  Loss 0.2823 (0.3249)  Prec@1 96.875 (95.989)  Prec@5 100.000 (99.703)
Epoch: [3][420/422]  Time 2.39 (2.30)  Loss 0.2733 (0.3249)  Prec@1 96.875 (95.979)  Prec@5 100.000 (99.705)
Test: [0/47]	Time 0.442 (0.442)	Loss 0.0393 (0.0393)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.097 (0.133)	Loss 0.1750 (0.0948)	Prec@1 96.094 (97.656)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.095 (0.115)	Loss 0.0711 (0.0964)	Prec@1 97.656 (97.731)	Prec@5 100.000 (99.963)
Test: [30/47]	Time 0.101 (0.109)	Loss 0.0333 (0.0834)	Prec@1 98.438 (97.883)	Prec@5 100.000 (99.975)
Test: [40/47]	Time 0.073 (0.104)	Loss 0.1091 (0.0804)	Prec@1 96.875 (97.847)	Prec@5 100.000 (99.981)
 * Prec@1 97.817 Prec@5 99.950
Epoch: [4][0/422]  Time 2.69 (2.69)  Loss 0.2847 (0.2847)  Prec@1 96.094 (96.094)  Prec@5 100.000 (100.000)
Epoch: [4][10/422]  Time 2.20 (2.29)  Loss 0.3482 (0.3171)  Prec@1 95.312 (96.307)  Prec@5 100.000 (99.858)
Epoch: [4][20/422]  Time 2.21 (2.26)  Loss 0.2929 (0.3110)  Prec@1 96.094 (96.354)  Prec@5 100.000 (99.814)
Epoch: [4][30/422]  Time 2.21 (2.25)  Loss 0.2899 (0.3153)  Prec@1 98.438 (96.119)  Prec@5 99.219 (99.723)
Epoch: [4][40/422]  Time 2.21 (2.24)  Loss 0.4624 (0.3245)  Prec@1 90.625 (95.827)  Prec@5 99.219 (99.676)
Epoch: [4][50/422]  Time 2.21 (2.24)  Loss 0.2691 (0.3222)  Prec@1 97.656 (95.803)  Prec@5 100.000 (99.724)
Epoch: [4][60/422]  Time 2.20 (2.23)  Loss 0.3288 (0.3205)  Prec@1 93.750 (95.850)  Prec@5 100.000 (99.731)
Epoch: [4][70/422]  Time 2.31 (2.23)  Loss 0.3779 (0.3216)  Prec@1 94.531 (95.940)  Prec@5 99.219 (99.725)
Epoch: [4][80/422]  Time 2.21 (2.23)  Loss 0.2623 (0.3217)  Prec@1 97.656 (95.920)  Prec@5 100.000 (99.730)
Epoch: [4][90/422]  Time 2.21 (2.23)  Loss 0.3443 (0.3208)  Prec@1 93.750 (95.956)  Prec@5 100.000 (99.742)
Epoch: [4][100/422]  Time 2.22 (2.23)  Loss 0.3691 (0.3217)  Prec@1 94.531 (95.947)  Prec@5 99.219 (99.745)
Epoch: [4][110/422]  Time 2.31 (2.23)  Loss 0.3860 (0.3209)  Prec@1 95.312 (96.016)  Prec@5 100.000 (99.740)
Epoch: [4][120/422]  Time 2.22 (2.23)  Loss 0.2663 (0.3190)  Prec@1 98.438 (96.094)  Prec@5 100.000 (99.729)
Epoch: [4][130/422]  Time 2.22 (2.23)  Loss 0.3259 (0.3172)  Prec@1 95.312 (96.153)  Prec@5 99.219 (99.738)
Epoch: [4][140/422]  Time 2.22 (2.23)  Loss 0.3181 (0.3164)  Prec@1 95.312 (96.205)  Prec@5 100.000 (99.751)
Epoch: [4][150/422]  Time 2.28 (2.23)  Loss 0.2916 (0.3157)  Prec@1 97.656 (96.239)  Prec@5 99.219 (99.736)
Epoch: [4][160/422]  Time 2.20 (2.23)  Loss 0.2999 (0.3141)  Prec@1 96.875 (96.264)  Prec@5 99.219 (99.748)
Epoch: [4][170/422]  Time 2.24 (2.23)  Loss 0.3060 (0.3147)  Prec@1 97.656 (96.235)  Prec@5 100.000 (99.735)
Epoch: [4][180/422]  Time 2.20 (2.23)  Loss 0.3062 (0.3148)  Prec@1 95.312 (96.232)  Prec@5 100.000 (99.737)
Epoch: [4][190/422]  Time 2.20 (2.23)  Loss 0.2572 (0.3135)  Prec@1 98.438 (96.282)  Prec@5 100.000 (99.730)
Epoch: [4][200/422]  Time 2.22 (2.23)  Loss 0.3868 (0.3133)  Prec@1 94.531 (96.308)  Prec@5 100.000 (99.724)
Epoch: [4][210/422]  Time 2.21 (2.23)  Loss 0.3217 (0.3129)  Prec@1 95.312 (96.305)  Prec@5 99.219 (99.730)
Epoch: [4][220/422]  Time 2.22 (2.23)  Loss 0.3237 (0.3130)  Prec@1 94.531 (96.288)  Prec@5 100.000 (99.731)
Epoch: [4][230/422]  Time 2.21 (2.23)  Loss 0.3272 (0.3126)  Prec@1 95.312 (96.293)  Prec@5 100.000 (99.719)
Epoch: [4][240/422]  Time 2.20 (2.23)  Loss 0.2707 (0.3129)  Prec@1 98.438 (96.291)  Prec@5 100.000 (99.711)
Epoch: [4][250/422]  Time 2.21 (2.23)  Loss 0.3553 (0.3133)  Prec@1 94.531 (96.284)  Prec@5 100.000 (99.707)
Epoch: [4][260/422]  Time 2.21 (2.23)  Loss 0.3547 (0.3140)  Prec@1 95.312 (96.264)  Prec@5 99.219 (99.707)
Epoch: [4][270/422]  Time 2.21 (2.23)  Loss 0.2762 (0.3136)  Prec@1 97.656 (96.270)  Prec@5 100.000 (99.715)
Epoch: [4][280/422]  Time 2.23 (2.23)  Loss 0.2780 (0.3131)  Prec@1 97.656 (96.283)  Prec@5 100.000 (99.725)
Epoch: [4][290/422]  Time 2.21 (2.23)  Loss 0.2716 (0.3133)  Prec@1 99.219 (96.287)  Prec@5 100.000 (99.726)
Epoch: [4][300/422]  Time 2.23 (2.23)  Loss 0.3109 (0.3138)  Prec@1 94.531 (96.278)  Prec@5 99.219 (99.720)
Epoch: [4][310/422]  Time 2.22 (2.23)  Loss 0.3463 (0.3132)  Prec@1 93.750 (96.277)  Prec@5 97.656 (99.716)
Epoch: [4][320/422]  Time 2.23 (2.23)  Loss 0.2777 (0.3130)  Prec@1 99.219 (96.286)  Prec@5 100.000 (99.718)
Epoch: [4][330/422]  Time 2.22 (2.23)  Loss 0.3112 (0.3127)  Prec@1 96.875 (96.290)  Prec@5 100.000 (99.712)
Epoch: [4][340/422]  Time 2.20 (2.23)  Loss 0.3397 (0.3123)  Prec@1 96.094 (96.309)  Prec@5 100.000 (99.711)
Epoch: [4][350/422]  Time 2.22 (2.23)  Loss 0.2307 (0.3122)  Prec@1 98.438 (96.314)  Prec@5 100.000 (99.715)
Epoch: [4][360/422]  Time 2.25 (2.23)  Loss 0.3026 (0.3120)  Prec@1 97.656 (96.319)  Prec@5 100.000 (99.716)
Epoch: [4][370/422]  Time 2.22 (2.23)  Loss 0.2413 (0.3114)  Prec@1 98.438 (96.344)  Prec@5 100.000 (99.716)
Epoch: [4][380/422]  Time 2.35 (2.23)  Loss 0.2849 (0.3108)  Prec@1 96.875 (96.354)  Prec@5 100.000 (99.719)
Epoch: [4][390/422]  Time 2.20 (2.23)  Loss 0.3132 (0.3111)  Prec@1 94.531 (96.344)  Prec@5 99.219 (99.718)
Epoch: [4][400/422]  Time 2.21 (2.23)  Loss 0.3144 (0.3107)  Prec@1 95.312 (96.347)  Prec@5 100.000 (99.721)
Epoch: [4][410/422]  Time 2.21 (2.23)  Loss 0.3581 (0.3105)  Prec@1 93.750 (96.360)  Prec@5 100.000 (99.722)
Epoch: [4][420/422]  Time 2.29 (2.23)  Loss 0.3356 (0.3108)  Prec@1 96.094 (96.365)  Prec@5 100.000 (99.724)
Test: [0/47]	Time 0.416 (0.416)	Loss 0.0177 (0.0177)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.097 (0.130)	Loss 0.1729 (0.0640)	Prec@1 96.875 (98.295)	Prec@5 99.219 (99.858)
Test: [20/47]	Time 0.095 (0.113)	Loss 0.1149 (0.0806)	Prec@1 96.094 (98.140)	Prec@5 100.000 (99.926)
Test: [30/47]	Time 0.095 (0.108)	Loss 0.1548 (0.0799)	Prec@1 96.094 (97.933)	Prec@5 99.219 (99.899)
Test: [40/47]	Time 0.074 (0.103)	Loss 0.0304 (0.0810)	Prec@1 98.438 (97.866)	Prec@5 100.000 (99.905)
 * Prec@1 97.950 Prec@5 99.917
Epoch: [5][0/422]  Time 2.58 (2.58)  Loss 0.2691 (0.2691)  Prec@1 97.656 (97.656)  Prec@5 100.000 (100.000)
Epoch: [5][10/422]  Time 2.23 (2.28)  Loss 0.2778 (0.2906)  Prec@1 96.094 (96.946)  Prec@5 100.000 (99.858)
Epoch: [5][20/422]  Time 2.20 (2.25)  Loss 0.3566 (0.3097)  Prec@1 93.750 (96.205)  Prec@5 100.000 (99.740)
Epoch: [5][30/422]  Time 2.23 (2.25)  Loss 0.3037 (0.3059)  Prec@1 97.656 (96.447)  Prec@5 100.000 (99.723)
Epoch: [5][40/422]  Time 2.23 (2.24)  Loss 0.2739 (0.3025)  Prec@1 96.875 (96.475)  Prec@5 100.000 (99.733)
Epoch: [5][50/422]  Time 2.35 (2.24)  Loss 0.2989 (0.3034)  Prec@1 97.656 (96.553)  Prec@5 100.000 (99.740)
Epoch: [5][60/422]  Time 2.21 (2.24)  Loss 0.3812 (0.3044)  Prec@1 95.312 (96.465)  Prec@5 99.219 (99.718)
Epoch: [5][70/422]  Time 2.23 (2.23)  Loss 0.3370 (0.3031)  Prec@1 96.094 (96.534)  Prec@5 100.000 (99.725)
Epoch: [5][80/422]  Time 2.21 (2.23)  Loss 0.3450 (0.3049)  Prec@1 94.531 (96.547)  Prec@5 100.000 (99.701)
Epoch: [5][90/422]  Time 2.33 (2.23)  Loss 0.3488 (0.3057)  Prec@1 95.312 (96.489)  Prec@5 100.000 (99.734)
Epoch: [5][100/422]  Time 2.22 (2.23)  Loss 0.3169 (0.3054)  Prec@1 96.094 (96.511)  Prec@5 99.219 (99.722)
Epoch: [5][110/422]  Time 2.19 (2.23)  Loss 0.3080 (0.3054)  Prec@1 95.312 (96.446)  Prec@5 100.000 (99.711)
Epoch: [5][120/422]  Time 2.21 (2.23)  Loss 0.2861 (0.3050)  Prec@1 97.656 (96.501)  Prec@5 99.219 (99.722)
Epoch: [5][130/422]  Time 2.21 (2.23)  Loss 0.3109 (0.3049)  Prec@1 97.656 (96.529)  Prec@5 100.000 (99.720)
Epoch: [5][140/422]  Time 2.21 (2.23)  Loss 0.2418 (0.3047)  Prec@1 99.219 (96.543)  Prec@5 100.000 (99.712)
Epoch: [5][150/422]  Time 2.20 (2.23)  Loss 0.2823 (0.3042)  Prec@1 98.438 (96.580)  Prec@5 100.000 (99.721)
Epoch: [5][160/422]  Time 2.22 (2.23)  Loss 0.3483 (0.3037)  Prec@1 94.531 (96.574)  Prec@5 98.438 (99.714)
Epoch: [5][170/422]  Time 2.22 (2.22)  Loss 0.3194 (0.3036)  Prec@1 96.875 (96.510)  Prec@5 100.000 (99.721)
Epoch: [5][180/422]  Time 2.38 (2.23)  Loss 0.2669 (0.3045)  Prec@1 97.656 (96.456)  Prec@5 100.000 (99.732)
Epoch: [5][190/422]  Time 2.41 (2.24)  Loss 0.2682 (0.3041)  Prec@1 97.656 (96.482)  Prec@5 100.000 (99.722)
Epoch: [5][200/422]  Time 2.21 (2.24)  Loss 0.2664 (0.3039)  Prec@1 98.438 (96.451)  Prec@5 100.000 (99.728)
Epoch: [5][210/422]  Time 2.33 (2.24)  Loss 0.2462 (0.3027)  Prec@1 97.656 (96.494)  Prec@5 100.000 (99.730)
Epoch: [5][220/422]  Time 2.22 (2.24)  Loss 0.2720 (0.3025)  Prec@1 96.094 (96.486)  Prec@5 100.000 (99.738)
Epoch: [5][230/422]  Time 2.22 (2.24)  Loss 0.3026 (0.3029)  Prec@1 97.656 (96.483)  Prec@5 100.000 (99.740)
Epoch: [5][240/422]  Time 2.23 (2.24)  Loss 0.3430 (0.3034)  Prec@1 95.312 (96.492)  Prec@5 99.219 (99.741)
Epoch: [5][250/422]  Time 2.22 (2.24)  Loss 0.2588 (0.3031)  Prec@1 98.438 (96.498)  Prec@5 100.000 (99.745)
Epoch: [5][260/422]  Time 2.23 (2.24)  Loss 0.2837 (0.3037)  Prec@1 96.875 (96.495)  Prec@5 100.000 (99.743)
Epoch: [5][270/422]  Time 2.21 (2.24)  Loss 0.2728 (0.3038)  Prec@1 96.875 (96.486)  Prec@5 100.000 (99.735)
Epoch: [5][280/422]  Time 2.21 (2.24)  Loss 0.2418 (0.3029)  Prec@1 97.656 (96.505)  Prec@5 100.000 (99.736)
Epoch: [5][290/422]  Time 2.23 (2.24)  Loss 0.3446 (0.3016)  Prec@1 96.094 (96.542)  Prec@5 100.000 (99.745)
Epoch: [5][300/422]  Time 2.21 (2.24)  Loss 0.3432 (0.3015)  Prec@1 95.312 (96.540)  Prec@5 100.000 (99.746)
Epoch: [5][310/422]  Time 2.22 (2.24)  Loss 0.2919 (0.3011)  Prec@1 96.094 (96.548)  Prec@5 99.219 (99.741)
Epoch: [5][320/422]  Time 2.24 (2.24)  Loss 0.2875 (0.3013)  Prec@1 96.094 (96.544)  Prec@5 100.000 (99.744)
Epoch: [5][330/422]  Time 2.22 (2.24)  Loss 0.4044 (0.3015)  Prec@1 93.750 (96.537)  Prec@5 97.656 (99.740)
Epoch: [5][340/422]  Time 2.21 (2.24)  Loss 0.3034 (0.3016)  Prec@1 95.312 (96.524)  Prec@5 100.000 (99.748)
Epoch: [5][350/422]  Time 2.23 (2.24)  Loss 0.3047 (0.3013)  Prec@1 95.312 (96.534)  Prec@5 100.000 (99.751)
Epoch: [5][360/422]  Time 2.22 (2.24)  Loss 0.3028 (0.3012)  Prec@1 97.656 (96.550)  Prec@5 99.219 (99.751)
Epoch: [5][370/422]  Time 2.22 (2.24)  Loss 0.2968 (0.3018)  Prec@1 96.094 (96.534)  Prec@5 100.000 (99.749)
Epoch: [5][380/422]  Time 2.21 (2.24)  Loss 0.2347 (0.3014)  Prec@1 97.656 (96.545)  Prec@5 100.000 (99.746)
Epoch: [5][390/422]  Time 2.21 (2.23)  Loss 0.2849 (0.3012)  Prec@1 95.312 (96.543)  Prec@5 100.000 (99.748)
Epoch: [5][400/422]  Time 2.21 (2.23)  Loss 0.2929 (0.3013)  Prec@1 95.312 (96.526)  Prec@5 99.219 (99.745)
Epoch: [5][410/422]  Time 2.20 (2.23)  Loss 0.3175 (0.3013)  Prec@1 96.094 (96.531)  Prec@5 98.438 (99.740)
Epoch: [5][420/422]  Time 2.20 (2.23)  Loss 0.2590 (0.3009)  Prec@1 98.438 (96.545)  Prec@5 100.000 (99.744)
Test: [0/47]	Time 0.415 (0.415)	Loss 0.1769 (0.1769)	Prec@1 96.875 (96.875)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.094 (0.130)	Loss 0.0210 (0.0565)	Prec@1 99.219 (98.580)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.094 (0.113)	Loss 0.2054 (0.0754)	Prec@1 96.875 (97.954)	Prec@5 99.219 (99.926)
Test: [30/47]	Time 0.094 (0.107)	Loss 0.0191 (0.0744)	Prec@1 100.000 (98.034)	Prec@5 100.000 (99.950)
Test: [40/47]	Time 0.073 (0.103)	Loss 0.1005 (0.0743)	Prec@1 97.656 (97.999)	Prec@5 100.000 (99.962)
 * Prec@1 97.950 Prec@5 99.967
Epoch: [6][0/422]  Time 2.69 (2.69)  Loss 0.2589 (0.2589)  Prec@1 97.656 (97.656)  Prec@5 100.000 (100.000)
Epoch: [6][10/422]  Time 2.22 (2.27)  Loss 0.3350 (0.2908)  Prec@1 95.312 (96.733)  Prec@5 100.000 (99.929)
Epoch: [6][20/422]  Time 2.21 (2.25)  Loss 0.2650 (0.2866)  Prec@1 96.094 (96.763)  Prec@5 100.000 (99.963)
Epoch: [6][30/422]  Time 2.21 (2.24)  Loss 0.2283 (0.2885)  Prec@1 98.438 (96.749)  Prec@5 100.000 (99.798)
Epoch: [6][40/422]  Time 2.23 (2.24)  Loss 0.2368 (0.2903)  Prec@1 98.438 (96.684)  Prec@5 100.000 (99.752)
Epoch: [6][50/422]  Time 2.21 (2.24)  Loss 0.2924 (0.2914)  Prec@1 96.875 (96.645)  Prec@5 100.000 (99.755)
Epoch: [6][60/422]  Time 2.22 (2.24)  Loss 0.2633 (0.2937)  Prec@1 98.438 (96.580)  Prec@5 100.000 (99.769)
Epoch: [6][70/422]  Time 2.23 (2.24)  Loss 0.2881 (0.2924)  Prec@1 97.656 (96.677)  Prec@5 100.000 (99.791)
Epoch: [6][80/422]  Time 2.20 (2.23)  Loss 0.3629 (0.2956)  Prec@1 96.094 (96.537)  Prec@5 99.219 (99.797)
Epoch: [6][90/422]  Time 2.21 (2.23)  Loss 0.2606 (0.2959)  Prec@1 97.656 (96.540)  Prec@5 99.219 (99.777)
Epoch: [6][100/422]  Time 2.21 (2.23)  Loss 0.3699 (0.2961)  Prec@1 94.531 (96.488)  Prec@5 100.000 (99.776)
Epoch: [6][110/422]  Time 2.43 (2.23)  Loss 0.3534 (0.2964)  Prec@1 94.531 (96.530)  Prec@5 100.000 (99.775)
Epoch: [6][120/422]  Time 2.40 (2.25)  Loss 0.2801 (0.2960)  Prec@1 96.875 (96.546)  Prec@5 100.000 (99.768)
Epoch: [6][130/422]  Time 2.52 (2.26)  Loss 0.2881 (0.2958)  Prec@1 96.094 (96.529)  Prec@5 100.000 (99.785)
Epoch: [6][140/422]  Time 2.35 (2.27)  Loss 0.2599 (0.2944)  Prec@1 96.094 (96.576)  Prec@5 100.000 (99.784)
Epoch: [6][150/422]  Time 2.35 (2.27)  Loss 0.3362 (0.2972)  Prec@1 94.531 (96.477)  Prec@5 99.219 (99.772)
Epoch: [6][160/422]  Time 2.44 (2.28)  Loss 0.2179 (0.2949)  Prec@1 100.000 (96.579)  Prec@5 100.000 (99.777)
Epoch: [6][170/422]  Time 2.33 (2.28)  Loss 0.2832 (0.2952)  Prec@1 97.656 (96.564)  Prec@5 100.000 (99.781)
Epoch: [6][180/422]  Time 2.34 (2.28)  Loss 0.3210 (0.2947)  Prec@1 96.094 (96.577)  Prec@5 99.219 (99.788)
Epoch: [6][190/422]  Time 2.35 (2.29)  Loss 0.2753 (0.2933)  Prec@1 96.094 (96.617)  Prec@5 100.000 (99.795)
Epoch: [6][200/422]  Time 2.36 (2.29)  Loss 0.3075 (0.2943)  Prec@1 96.094 (96.611)  Prec@5 99.219 (99.778)
Epoch: [6][210/422]  Time 2.34 (2.29)  Loss 0.3456 (0.2938)  Prec@1 95.312 (96.645)  Prec@5 100.000 (99.785)
Epoch: [6][220/422]  Time 2.34 (2.30)  Loss 0.3446 (0.2948)  Prec@1 93.750 (96.638)  Prec@5 100.000 (99.774)
Epoch: [6][230/422]  Time 2.34 (2.30)  Loss 0.3160 (0.2954)  Prec@1 94.531 (96.628)  Prec@5 99.219 (99.773)
Epoch: [6][240/422]  Time 2.34 (2.30)  Loss 0.3676 (0.2959)  Prec@1 93.750 (96.616)  Prec@5 100.000 (99.767)
Epoch: [6][250/422]  Time 2.35 (2.30)  Loss 0.2776 (0.2961)  Prec@1 97.656 (96.604)  Prec@5 100.000 (99.767)
Epoch: [6][260/422]  Time 2.35 (2.31)  Loss 0.3724 (0.2963)  Prec@1 94.531 (96.582)  Prec@5 100.000 (99.773)
Epoch: [6][270/422]  Time 2.33 (2.31)  Loss 0.3509 (0.2960)  Prec@1 95.312 (96.592)  Prec@5 99.219 (99.769)
Epoch: [6][280/422]  Time 2.33 (2.31)  Loss 0.3521 (0.2956)  Prec@1 93.750 (96.611)  Prec@5 100.000 (99.769)
Epoch: [6][290/422]  Time 2.35 (2.31)  Loss 0.2380 (0.2967)  Prec@1 100.000 (96.582)  Prec@5 100.000 (99.761)
Epoch: [6][300/422]  Time 2.49 (2.31)  Loss 0.3569 (0.2963)  Prec@1 95.312 (96.592)  Prec@5 100.000 (99.756)
Epoch: [6][310/422]  Time 2.35 (2.31)  Loss 0.3222 (0.2960)  Prec@1 95.312 (96.596)  Prec@5 100.000 (99.754)
Epoch: [6][320/422]  Time 2.33 (2.31)  Loss 0.2434 (0.2957)  Prec@1 96.094 (96.605)  Prec@5 100.000 (99.752)
Epoch: [6][330/422]  Time 2.32 (2.32)  Loss 0.2037 (0.2952)  Prec@1 100.000 (96.625)  Prec@5 100.000 (99.750)
Epoch: [6][340/422]  Time 2.34 (2.32)  Loss 0.3108 (0.2952)  Prec@1 95.312 (96.616)  Prec@5 100.000 (99.750)
Epoch: [6][350/422]  Time 2.34 (2.32)  Loss 0.2629 (0.2951)  Prec@1 97.656 (96.617)  Prec@5 100.000 (99.744)
Epoch: [6][360/422]  Time 2.35 (2.32)  Loss 0.2547 (0.2945)  Prec@1 100.000 (96.633)  Prec@5 100.000 (99.745)
Epoch: [6][370/422]  Time 2.37 (2.32)  Loss 0.2502 (0.2948)  Prec@1 96.875 (96.620)  Prec@5 100.000 (99.745)
Epoch: [6][380/422]  Time 2.41 (2.32)  Loss 0.3108 (0.2946)  Prec@1 96.875 (96.643)  Prec@5 100.000 (99.742)
Epoch: [6][390/422]  Time 2.40 (2.32)  Loss 0.3280 (0.2945)  Prec@1 94.531 (96.641)  Prec@5 99.219 (99.738)
Epoch: [6][400/422]  Time 2.21 (2.33)  Loss 0.2911 (0.2937)  Prec@1 96.094 (96.657)  Prec@5 100.000 (99.743)
Epoch: [6][410/422]  Time 2.22 (2.32)  Loss 0.2969 (0.2938)  Prec@1 94.531 (96.645)  Prec@5 100.000 (99.745)
Epoch: [6][420/422]  Time 2.19 (2.32)  Loss 0.3961 (0.2942)  Prec@1 94.531 (96.628)  Prec@5 99.219 (99.748)
Test: [0/47]	Time 0.423 (0.423)	Loss 0.0165 (0.0165)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.092 (0.129)	Loss 0.0513 (0.0750)	Prec@1 97.656 (97.798)	Prec@5 100.000 (99.929)
Test: [20/47]	Time 0.091 (0.113)	Loss 0.1343 (0.0850)	Prec@1 97.656 (97.731)	Prec@5 100.000 (99.963)
Test: [30/47]	Time 0.093 (0.108)	Loss 0.0991 (0.0779)	Prec@1 97.656 (97.782)	Prec@5 99.219 (99.950)
Test: [40/47]	Time 0.074 (0.104)	Loss 0.0377 (0.0721)	Prec@1 98.438 (97.923)	Prec@5 100.000 (99.943)
 * Prec@1 97.950 Prec@5 99.950
Epoch: [7][0/422]  Time 2.73 (2.73)  Loss 0.2746 (0.2746)  Prec@1 96.094 (96.094)  Prec@5 100.000 (100.000)
Epoch: [7][10/422]  Time 2.22 (2.35)  Loss 0.2369 (0.2849)  Prec@1 98.438 (96.520)  Prec@5 100.000 (99.858)
Epoch: [7][20/422]  Time 2.21 (2.30)  Loss 0.3015 (0.2884)  Prec@1 96.094 (96.503)  Prec@5 99.219 (99.814)
Epoch: [7][30/422]  Time 2.23 (2.27)  Loss 0.2650 (0.2928)  Prec@1 97.656 (96.522)  Prec@5 100.000 (99.723)
Epoch: [7][40/422]  Time 2.21 (2.26)  Loss 0.3132 (0.2935)  Prec@1 96.875 (96.532)  Prec@5 100.000 (99.771)
Epoch: [7][50/422]  Time 2.22 (2.25)  Loss 0.2452 (0.2877)  Prec@1 97.656 (96.630)  Prec@5 100.000 (99.801)
Epoch: [7][60/422]  Time 2.20 (2.25)  Loss 0.3475 (0.2877)  Prec@1 95.312 (96.619)  Prec@5 99.219 (99.808)
Epoch: [7][70/422]  Time 2.40 (2.25)  Loss 0.3616 (0.2887)  Prec@1 95.312 (96.622)  Prec@5 98.438 (99.791)
Epoch: [7][80/422]  Time 2.40 (2.27)  Loss 0.2894 (0.2886)  Prec@1 97.656 (96.672)  Prec@5 99.219 (99.769)
Epoch: [7][90/422]  Time 2.43 (2.29)  Loss 0.2818 (0.2887)  Prec@1 96.875 (96.669)  Prec@5 100.000 (99.760)
Epoch: [7][100/422]  Time 2.41 (2.30)  Loss 0.2382 (0.2876)  Prec@1 97.656 (96.682)  Prec@5 100.000 (99.768)
Epoch: [7][110/422]  Time 2.38 (2.31)  Loss 0.3070 (0.2866)  Prec@1 96.094 (96.727)  Prec@5 99.219 (99.775)
Epoch: [7][120/422]  Time 2.40 (2.32)  Loss 0.2389 (0.2847)  Prec@1 97.656 (96.778)  Prec@5 100.000 (99.774)
Epoch: [7][130/422]  Time 2.40 (2.32)  Loss 0.1938 (0.2846)  Prec@1 100.000 (96.786)  Prec@5 100.000 (99.779)
Epoch: [7][140/422]  Time 2.41 (2.33)  Loss 0.2513 (0.2832)  Prec@1 98.438 (96.831)  Prec@5 100.000 (99.784)
Epoch: [7][150/422]  Time 2.38 (2.33)  Loss 0.2808 (0.2821)  Prec@1 97.656 (96.901)  Prec@5 100.000 (99.798)
Epoch: [7][160/422]  Time 2.41 (2.34)  Loss 0.2926 (0.2808)  Prec@1 96.875 (96.914)  Prec@5 100.000 (99.801)
Epoch: [7][170/422]  Time 2.22 (2.34)  Loss 0.2805 (0.2812)  Prec@1 96.875 (96.934)  Prec@5 100.000 (99.799)
Epoch: [7][180/422]  Time 2.21 (2.33)  Loss 0.2907 (0.2817)  Prec@1 96.875 (96.935)  Prec@5 100.000 (99.810)
Epoch: [7][190/422]  Time 2.22 (2.32)  Loss 0.2656 (0.2821)  Prec@1 97.656 (96.957)  Prec@5 100.000 (99.795)
Epoch: [7][200/422]  Time 2.21 (2.32)  Loss 0.3560 (0.2831)  Prec@1 95.312 (96.918)  Prec@5 100.000 (99.794)
Epoch: [7][210/422]  Time 2.21 (2.31)  Loss 0.3202 (0.2843)  Prec@1 95.312 (96.871)  Prec@5 99.219 (99.785)
Epoch: [7][220/422]  Time 2.22 (2.31)  Loss 0.2883 (0.2844)  Prec@1 96.875 (96.868)  Prec@5 100.000 (99.781)
Epoch: [7][230/422]  Time 2.22 (2.31)  Loss 0.2350 (0.2840)  Prec@1 97.656 (96.882)  Prec@5 100.000 (99.787)
Epoch: [7][240/422]  Time 2.21 (2.30)  Loss 0.2728 (0.2830)  Prec@1 97.656 (96.920)  Prec@5 100.000 (99.796)
Epoch: [7][250/422]  Time 2.21 (2.30)  Loss 0.3052 (0.2838)  Prec@1 97.656 (96.909)  Prec@5 100.000 (99.801)
Epoch: [7][260/422]  Time 2.23 (2.30)  Loss 0.3298 (0.2836)  Prec@1 95.312 (96.920)  Prec@5 100.000 (99.802)
Epoch: [7][270/422]  Time 2.22 (2.30)  Loss 0.3318 (0.2838)  Prec@1 96.094 (96.927)  Prec@5 99.219 (99.801)
Epoch: [7][280/422]  Time 2.20 (2.29)  Loss 0.2665 (0.2841)  Prec@1 98.438 (96.933)  Prec@5 100.000 (99.794)
Epoch: [7][290/422]  Time 2.23 (2.29)  Loss 0.2182 (0.2845)  Prec@1 99.219 (96.926)  Prec@5 100.000 (99.796)
Epoch: [7][300/422]  Time 2.26 (2.29)  Loss 0.2269 (0.2844)  Prec@1 99.219 (96.927)  Prec@5 100.000 (99.800)
Epoch: [7][310/422]  Time 2.21 (2.29)  Loss 0.3539 (0.2842)  Prec@1 97.656 (96.938)  Prec@5 99.219 (99.794)
Epoch: [7][320/422]  Time 2.35 (2.28)  Loss 0.3441 (0.2844)  Prec@1 96.094 (96.924)  Prec@5 100.000 (99.788)
Epoch: [7][330/422]  Time 2.20 (2.28)  Loss 0.2848 (0.2843)  Prec@1 98.438 (96.927)  Prec@5 100.000 (99.785)
Epoch: [7][340/422]  Time 2.20 (2.28)  Loss 0.3358 (0.2841)  Prec@1 95.312 (96.937)  Prec@5 100.000 (99.789)
Epoch: [7][350/422]  Time 2.23 (2.28)  Loss 0.2023 (0.2843)  Prec@1 97.656 (96.906)  Prec@5 100.000 (99.791)
Epoch: [7][360/422]  Time 2.36 (2.28)  Loss 0.2790 (0.2846)  Prec@1 99.219 (96.912)  Prec@5 99.219 (99.781)
Epoch: [7][370/422]  Time 2.21 (2.28)  Loss 0.2608 (0.2847)  Prec@1 97.656 (96.919)  Prec@5 100.000 (99.787)
Epoch: [7][380/422]  Time 2.19 (2.27)  Loss 0.2835 (0.2852)  Prec@1 97.656 (96.898)  Prec@5 100.000 (99.785)
Epoch: [7][390/422]  Time 2.20 (2.27)  Loss 0.2613 (0.2852)  Prec@1 98.438 (96.889)  Prec@5 99.219 (99.782)
Epoch: [7][400/422]  Time 2.22 (2.27)  Loss 0.3022 (0.2853)  Prec@1 96.875 (96.883)  Prec@5 99.219 (99.780)
Epoch: [7][410/422]  Time 2.21 (2.27)  Loss 0.3543 (0.2858)  Prec@1 95.312 (96.871)  Prec@5 99.219 (99.780)
Epoch: [7][420/422]  Time 2.19 (2.27)  Loss 0.2378 (0.2861)  Prec@1 96.875 (96.843)  Prec@5 100.000 (99.777)
Test: [0/47]	Time 0.410 (0.410)	Loss 0.0492 (0.0492)	Prec@1 97.656 (97.656)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.092 (0.130)	Loss 0.0567 (0.0541)	Prec@1 98.438 (98.295)	Prec@5 100.000 (99.929)
Test: [20/47]	Time 0.092 (0.114)	Loss 0.0713 (0.0553)	Prec@1 98.438 (98.400)	Prec@5 100.000 (99.926)
Test: [30/47]	Time 0.092 (0.108)	Loss 0.0401 (0.0555)	Prec@1 99.219 (98.362)	Prec@5 99.219 (99.924)
Test: [40/47]	Time 0.071 (0.103)	Loss 0.0706 (0.0669)	Prec@1 97.656 (98.190)	Prec@5 100.000 (99.924)
 * Prec@1 98.117 Prec@5 99.933
Epoch: [8][0/422]  Time 2.62 (2.62)  Loss 0.2438 (0.2438)  Prec@1 97.656 (97.656)  Prec@5 100.000 (100.000)
Epoch: [8][10/422]  Time 2.21 (2.27)  Loss 0.3125 (0.2817)  Prec@1 96.094 (96.875)  Prec@5 99.219 (99.787)
Epoch: [8][20/422]  Time 2.20 (2.25)  Loss 0.3322 (0.2822)  Prec@1 96.875 (96.912)  Prec@5 100.000 (99.814)
Epoch: [8][30/422]  Time 2.21 (2.24)  Loss 0.3491 (0.2797)  Prec@1 93.750 (96.976)  Prec@5 100.000 (99.824)
Epoch: [8][40/422]  Time 2.20 (2.23)  Loss 0.4231 (0.2853)  Prec@1 92.969 (96.799)  Prec@5 98.438 (99.829)
Epoch: [8][50/422]  Time 2.19 (2.23)  Loss 0.3792 (0.2914)  Prec@1 93.750 (96.584)  Prec@5 99.219 (99.786)
Epoch: [8][60/422]  Time 2.34 (2.23)  Loss 0.2171 (0.2912)  Prec@1 97.656 (96.619)  Prec@5 100.000 (99.769)
Epoch: [8][70/422]  Time 2.21 (2.23)  Loss 0.2384 (0.2919)  Prec@1 99.219 (96.622)  Prec@5 100.000 (99.791)
Epoch: [8][80/422]  Time 2.21 (2.23)  Loss 0.2508 (0.2913)  Prec@1 96.875 (96.653)  Prec@5 100.000 (99.797)
Epoch: [8][90/422]  Time 2.21 (2.23)  Loss 0.2959 (0.2921)  Prec@1 96.875 (96.678)  Prec@5 99.219 (99.785)
Epoch: [8][100/422]  Time 2.20 (2.23)  Loss 0.3078 (0.2920)  Prec@1 94.531 (96.674)  Prec@5 100.000 (99.807)
Epoch: [8][110/422]  Time 2.23 (2.23)  Loss 0.3080 (0.2919)  Prec@1 95.312 (96.664)  Prec@5 100.000 (99.817)
Epoch: [8][120/422]  Time 2.20 (2.23)  Loss 0.3121 (0.2907)  Prec@1 96.094 (96.688)  Prec@5 100.000 (99.806)
Epoch: [8][130/422]  Time 2.42 (2.23)  Loss 0.2555 (0.2905)  Prec@1 97.656 (96.678)  Prec@5 100.000 (99.785)
Epoch: [8][140/422]  Time 2.38 (2.25)  Loss 0.2793 (0.2896)  Prec@1 97.656 (96.703)  Prec@5 100.000 (99.789)
Epoch: [8][150/422]  Time 2.47 (2.26)  Loss 0.3345 (0.2899)  Prec@1 97.656 (96.715)  Prec@5 99.219 (99.783)
Epoch: [8][160/422]  Time 2.39 (2.27)  Loss 0.2566 (0.2898)  Prec@1 98.438 (96.729)  Prec@5 100.000 (99.782)
Epoch: [8][170/422]  Time 2.39 (2.27)  Loss 0.2797 (0.2893)  Prec@1 96.094 (96.752)  Prec@5 99.219 (99.781)
Epoch: [8][180/422]  Time 2.33 (2.28)  Loss 0.2672 (0.2898)  Prec@1 96.875 (96.741)  Prec@5 100.000 (99.776)
Epoch: [8][190/422]  Time 2.33 (2.28)  Loss 0.2981 (0.2888)  Prec@1 96.875 (96.781)  Prec@5 98.438 (99.779)
Epoch: [8][200/422]  Time 2.33 (2.29)  Loss 0.3574 (0.2884)  Prec@1 94.531 (96.801)  Prec@5 98.438 (99.775)
Epoch: [8][210/422]  Time 2.33 (2.29)  Loss 0.3217 (0.2881)  Prec@1 93.750 (96.790)  Prec@5 100.000 (99.778)
Epoch: [8][220/422]  Time 2.31 (2.29)  Loss 0.2717 (0.2879)  Prec@1 96.875 (96.790)  Prec@5 100.000 (99.770)
Epoch: [8][230/422]  Time 2.34 (2.29)  Loss 0.2164 (0.2897)  Prec@1 98.438 (96.753)  Prec@5 100.000 (99.750)
Epoch: [8][240/422]  Time 2.34 (2.30)  Loss 0.2864 (0.2897)  Prec@1 96.094 (96.762)  Prec@5 99.219 (99.747)
Epoch: [8][250/422]  Time 2.33 (2.30)  Loss 0.2786 (0.2897)  Prec@1 97.656 (96.763)  Prec@5 100.000 (99.754)
Epoch: [8][260/422]  Time 2.33 (2.30)  Loss 0.3175 (0.2898)  Prec@1 96.094 (96.740)  Prec@5 99.219 (99.752)
Epoch: [8][270/422]  Time 2.34 (2.30)  Loss 0.2980 (0.2896)  Prec@1 97.656 (96.763)  Prec@5 100.000 (99.755)
Epoch: [8][280/422]  Time 2.21 (2.30)  Loss 0.2885 (0.2882)  Prec@1 96.875 (96.803)  Prec@5 100.000 (99.764)
Epoch: [8][290/422]  Time 2.20 (2.30)  Loss 0.2361 (0.2871)  Prec@1 96.875 (96.813)  Prec@5 100.000 (99.766)
Epoch: [8][300/422]  Time 2.21 (2.30)  Loss 0.3097 (0.2869)  Prec@1 96.094 (96.813)  Prec@5 100.000 (99.774)
Epoch: [8][310/422]  Time 2.20 (2.29)  Loss 0.3384 (0.2868)  Prec@1 97.656 (96.827)  Prec@5 100.000 (99.781)
Epoch: [8][320/422]  Time 2.24 (2.29)  Loss 0.2880 (0.2863)  Prec@1 96.875 (96.826)  Prec@5 100.000 (99.786)
Epoch: [8][330/422]  Time 2.21 (2.29)  Loss 0.2610 (0.2861)  Prec@1 98.438 (96.818)  Prec@5 100.000 (99.788)
Epoch: [8][340/422]  Time 2.23 (2.29)  Loss 0.2733 (0.2853)  Prec@1 97.656 (96.843)  Prec@5 99.219 (99.787)
Epoch: [8][350/422]  Time 2.22 (2.29)  Loss 0.2446 (0.2848)  Prec@1 97.656 (96.851)  Prec@5 100.000 (99.791)
Epoch: [8][360/422]  Time 2.22 (2.28)  Loss 0.3094 (0.2843)  Prec@1 95.312 (96.856)  Prec@5 100.000 (99.797)
Epoch: [8][370/422]  Time 2.22 (2.28)  Loss 0.2431 (0.2846)  Prec@1 98.438 (96.860)  Prec@5 100.000 (99.794)
Epoch: [8][380/422]  Time 2.22 (2.28)  Loss 0.2638 (0.2847)  Prec@1 97.656 (96.854)  Prec@5 100.000 (99.795)
Epoch: [8][390/422]  Time 2.23 (2.28)  Loss 0.3005 (0.2849)  Prec@1 96.875 (96.835)  Prec@5 100.000 (99.794)
Epoch: [8][400/422]  Time 2.23 (2.28)  Loss 0.2274 (0.2846)  Prec@1 97.656 (96.840)  Prec@5 100.000 (99.792)
Epoch: [8][410/422]  Time 2.22 (2.28)  Loss 0.2456 (0.2841)  Prec@1 98.438 (96.852)  Prec@5 100.000 (99.789)
Epoch: [8][420/422]  Time 2.19 (2.28)  Loss 0.2632 (0.2836)  Prec@1 96.875 (96.868)  Prec@5 98.438 (99.785)
Test: [0/47]	Time 0.338 (0.338)	Loss 0.0161 (0.0161)	Prec@1 99.219 (99.219)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.094 (0.124)	Loss 0.0427 (0.0608)	Prec@1 97.656 (98.509)	Prec@5 100.000 (99.929)
Test: [20/47]	Time 0.095 (0.110)	Loss 0.0349 (0.0645)	Prec@1 98.438 (98.363)	Prec@5 100.000 (99.963)
Test: [30/47]	Time 0.091 (0.105)	Loss 0.0649 (0.0730)	Prec@1 99.219 (98.135)	Prec@5 99.219 (99.950)
Test: [40/47]	Time 0.076 (0.101)	Loss 0.0678 (0.0695)	Prec@1 96.094 (98.171)	Prec@5 100.000 (99.962)
 * Prec@1 98.117 Prec@5 99.967
Epoch: [9][0/422]  Time 2.79 (2.79)  Loss 0.2849 (0.2849)  Prec@1 96.875 (96.875)  Prec@5 100.000 (100.000)
Epoch: [9][10/422]  Time 2.21 (2.38)  Loss 0.3189 (0.2943)  Prec@1 94.531 (96.165)  Prec@5 100.000 (99.716)
Epoch: [9][20/422]  Time 2.22 (2.31)  Loss 0.2546 (0.2803)  Prec@1 97.656 (96.689)  Prec@5 100.000 (99.777)
Epoch: [9][30/422]  Time 2.22 (2.28)  Loss 0.3277 (0.2762)  Prec@1 96.875 (97.001)  Prec@5 99.219 (99.798)
Epoch: [9][40/422]  Time 2.23 (2.27)  Loss 0.2634 (0.2780)  Prec@1 98.438 (96.932)  Prec@5 100.000 (99.809)
Epoch: [9][50/422]  Time 2.20 (2.26)  Loss 0.2311 (0.2774)  Prec@1 100.000 (96.998)  Prec@5 100.000 (99.831)
Epoch: [9][60/422]  Time 2.21 (2.26)  Loss 0.2870 (0.2786)  Prec@1 95.312 (97.003)  Prec@5 100.000 (99.795)
Epoch: [9][70/422]  Time 2.21 (2.25)  Loss 0.2544 (0.2743)  Prec@1 97.656 (97.128)  Prec@5 100.000 (99.824)
Epoch: [9][80/422]  Time 2.21 (2.25)  Loss 0.2256 (0.2710)  Prec@1 97.656 (97.213)  Prec@5 100.000 (99.846)
Epoch: [9][90/422]  Time 2.23 (2.25)  Loss 0.3034 (0.2691)  Prec@1 97.656 (97.210)  Prec@5 100.000 (99.854)
Epoch: [9][100/422]  Time 2.21 (2.24)  Loss 0.2908 (0.2694)  Prec@1 97.656 (97.153)  Prec@5 100.000 (99.861)
Epoch: [9][110/422]  Time 2.20 (2.24)  Loss 0.2515 (0.2701)  Prec@1 97.656 (97.164)  Prec@5 100.000 (99.859)
Epoch: [9][120/422]  Time 2.22 (2.24)  Loss 0.3424 (0.2737)  Prec@1 95.312 (97.036)  Prec@5 99.219 (99.839)
Epoch: [9][130/422]  Time 2.21 (2.24)  Loss 0.2645 (0.2751)  Prec@1 97.656 (97.006)  Prec@5 100.000 (99.839)
Epoch: [9][140/422]  Time 2.22 (2.24)  Loss 0.3358 (0.2745)  Prec@1 96.094 (97.025)  Prec@5 100.000 (99.839)
Epoch: [9][150/422]  Time 2.22 (2.24)  Loss 0.2540 (0.2735)  Prec@1 96.875 (97.051)  Prec@5 100.000 (99.834)
Epoch: [9][160/422]  Time 2.21 (2.24)  Loss 0.3005 (0.2743)  Prec@1 98.438 (97.025)  Prec@5 99.219 (99.840)
Epoch: [9][170/422]  Time 2.22 (2.24)  Loss 0.3176 (0.2743)  Prec@1 96.875 (97.017)  Prec@5 100.000 (99.845)
Epoch: [9][180/422]  Time 2.19 (2.24)  Loss 0.3241 (0.2745)  Prec@1 96.875 (97.022)  Prec@5 99.219 (99.845)
Epoch: [9][190/422]  Time 2.32 (2.23)  Loss 0.2958 (0.2765)  Prec@1 97.656 (96.973)  Prec@5 99.219 (99.832)
Epoch: [9][200/422]  Time 2.21 (2.23)  Loss 0.2511 (0.2773)  Prec@1 96.094 (96.949)  Prec@5 100.000 (99.821)
Epoch: [9][210/422]  Time 2.22 (2.23)  Loss 0.2876 (0.2770)  Prec@1 98.438 (96.953)  Prec@5 99.219 (99.822)
Epoch: [9][220/422]  Time 2.22 (2.23)  Loss 0.3188 (0.2778)  Prec@1 95.312 (96.932)  Prec@5 99.219 (99.820)
Epoch: [9][230/422]  Time 2.33 (2.23)  Loss 0.3146 (0.2776)  Prec@1 95.312 (96.943)  Prec@5 100.000 (99.824)
Epoch: [9][240/422]  Time 2.22 (2.23)  Loss 0.3037 (0.2784)  Prec@1 97.656 (96.943)  Prec@5 98.438 (99.815)
Epoch: [9][250/422]  Time 2.20 (2.23)  Loss 0.3669 (0.2789)  Prec@1 96.875 (96.947)  Prec@5 99.219 (99.816)
Epoch: [9][260/422]  Time 2.21 (2.23)  Loss 0.2672 (0.2792)  Prec@1 97.656 (96.959)  Prec@5 100.000 (99.817)
Epoch: [9][270/422]  Time 2.27 (2.23)  Loss 0.2538 (0.2792)  Prec@1 96.875 (96.947)  Prec@5 100.000 (99.824)
Epoch: [9][280/422]  Time 2.23 (2.23)  Loss 0.3433 (0.2794)  Prec@1 95.312 (96.931)  Prec@5 100.000 (99.825)
Epoch: [9][290/422]  Time 2.22 (2.23)  Loss 0.2858 (0.2795)  Prec@1 97.656 (96.931)  Prec@5 99.219 (99.823)
Epoch: [9][300/422]  Time 2.23 (2.23)  Loss 0.3082 (0.2792)  Prec@1 96.094 (96.930)  Prec@5 99.219 (99.818)
Epoch: [9][310/422]  Time 2.21 (2.23)  Loss 0.4388 (0.2793)  Prec@1 93.750 (96.935)  Prec@5 98.438 (99.814)
Epoch: [9][320/422]  Time 2.22 (2.23)  Loss 0.2647 (0.2794)  Prec@1 97.656 (96.926)  Prec@5 100.000 (99.817)
Epoch: [9][330/422]  Time 2.21 (2.23)  Loss 0.2768 (0.2799)  Prec@1 96.875 (96.913)  Prec@5 100.000 (99.811)
Epoch: [9][340/422]  Time 2.23 (2.23)  Loss 0.2527 (0.2794)  Prec@1 98.438 (96.923)  Prec@5 100.000 (99.814)
Epoch: [9][350/422]  Time 2.22 (2.23)  Loss 0.2985 (0.2795)  Prec@1 95.312 (96.933)  Prec@5 99.219 (99.815)
Epoch: [9][360/422]  Time 2.22 (2.23)  Loss 0.3945 (0.2798)  Prec@1 93.750 (96.923)  Prec@5 100.000 (99.816)
Epoch: [9][370/422]  Time 2.22 (2.23)  Loss 0.3088 (0.2803)  Prec@1 95.312 (96.919)  Prec@5 100.000 (99.817)
Epoch: [9][380/422]  Time 2.19 (2.23)  Loss 0.2179 (0.2800)  Prec@1 99.219 (96.939)  Prec@5 100.000 (99.822)
Epoch: [9][390/422]  Time 2.19 (2.23)  Loss 0.2531 (0.2803)  Prec@1 96.875 (96.925)  Prec@5 100.000 (99.822)
Epoch: [9][400/422]  Time 2.19 (2.23)  Loss 0.2346 (0.2804)  Prec@1 97.656 (96.914)  Prec@5 100.000 (99.825)
Epoch: [9][410/422]  Time 2.19 (2.23)  Loss 0.2368 (0.2808)  Prec@1 97.656 (96.913)  Prec@5 100.000 (99.823)
Epoch: [9][420/422]  Time 2.18 (2.23)  Loss 0.2703 (0.2810)  Prec@1 98.438 (96.905)  Prec@5 100.000 (99.826)
Test: [0/47]	Time 0.352 (0.352)	Loss 0.0569 (0.0569)	Prec@1 98.438 (98.438)	Prec@5 100.000 (100.000)
Test: [10/47]	Time 0.095 (0.124)	Loss 0.0880 (0.0692)	Prec@1 96.875 (98.082)	Prec@5 100.000 (100.000)
Test: [20/47]	Time 0.095 (0.110)	Loss 0.0891 (0.0659)	Prec@1 97.656 (98.214)	Prec@5 100.000 (99.963)
Test: [30/47]	Time 0.095 (0.105)	Loss 0.0956 (0.0704)	Prec@1 96.094 (98.009)	Prec@5 100.000 (99.950)
Test: [40/47]	Time 0.075 (0.102)	Loss 0.0111 (0.0678)	Prec@1 99.219 (98.152)	Prec@5 100.000 (99.943)
 * Prec@1 98.117 Prec@5 99.950
Training done
Test: [0/79]	Time 0.284 (0.284)	Loss 0.0077 (0.0077)	Prec@1 100.000 (100.000)	Prec@5 100.000 (100.000)
Test: [10/79]	Time 0.095 (0.118)	Loss 0.1241 (0.0781)	Prec@1 96.094 (97.869)	Prec@5 100.000 (100.000)
Test: [20/79]	Time 0.095 (0.107)	Loss 0.1098 (0.0892)	Prec@1 97.656 (97.470)	Prec@5 100.000 (100.000)
Test: [30/79]	Time 0.093 (0.103)	Loss 0.1332 (0.0880)	Prec@1 96.094 (97.530)	Prec@5 100.000 (100.000)
Test: [40/79]	Time 0.094 (0.101)	Loss 0.0142 (0.0834)	Prec@1 99.219 (97.618)	Prec@5 100.000 (100.000)
Test: [50/79]	Time 0.095 (0.100)	Loss 0.0533 (0.0726)	Prec@1 99.219 (97.886)	Prec@5 100.000 (100.000)
Test: [60/79]	Time 0.095 (0.099)	Loss 0.0022 (0.0637)	Prec@1 100.000 (98.143)	Prec@5 100.000 (99.987)
Test: [70/79]	Time 0.077 (0.099)	Loss 0.0859 (0.0582)	Prec@1 96.875 (98.283)	Prec@5 100.000 (99.989)
 * Prec@1 98.290 Prec@5 99.990
Initializing GSENN wrapper.
Computing train data stats...
**** Performing black-box lipschitz estimation over subset of dataset ****
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1336
Function value obtained: -0.0099
Current minimum: -0.0099
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0079
Current minimum: -0.0099
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0100
Current minimum: -0.0100
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0120
Current minimum: -0.0120
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1301
Function value obtained: -0.0060
Current minimum: -0.0120
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1295
Function value obtained: -0.0025
Current minimum: -0.0120
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1294
Function value obtained: -0.0056
Current minimum: -0.0120
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1283
Function value obtained: -0.0012
Current minimum: -0.0120
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1285
Function value obtained: -0.0020
Current minimum: -0.0120
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 10.1894
Function value obtained: -0.0027
Current minimum: -0.0120
0.011981264110632724 0.1300951555476482
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1328
Function value obtained: -0.0207
Current minimum: -0.0207
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1315
Function value obtained: -0.0198
Current minimum: -0.0207
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1307
Function value obtained: -0.0116
Current minimum: -0.0207
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1307
Function value obtained: -0.0184
Current minimum: -0.0207
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0177
Current minimum: -0.0207
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1301
Function value obtained: -0.0127
Current minimum: -0.0207
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0107
Current minimum: -0.0207
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1290
Function value obtained: -0.0140
Current minimum: -0.0207
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1285
Function value obtained: -0.0114
Current minimum: -0.0207
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.1284
Function value obtained: -0.0102
Current minimum: -0.0207
0.02067253099161481 0.13564016632159673
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1308
Function value obtained: -0.0288
Current minimum: -0.0288
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1308
Function value obtained: -0.0445
Current minimum: -0.0445
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0232
Current minimum: -0.0445
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0233
Current minimum: -0.0445
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1302
Function value obtained: -0.0216
Current minimum: -0.0445
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1306
Function value obtained: -0.0439
Current minimum: -0.0445
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0231
Current minimum: -0.0445
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1279
Function value obtained: -0.0285
Current minimum: -0.0445
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1288
Function value obtained: -0.0330
Current minimum: -0.0445
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 8.9501
Function value obtained: -0.0370
Current minimum: -0.0445
0.044509661705191945 0.1329362705434797
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1293
Function value obtained: -0.0461
Current minimum: -0.0461
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1294
Function value obtained: -0.0520
Current minimum: -0.0520
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1287
Function value obtained: -0.0407
Current minimum: -0.0520
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1291
Function value obtained: -0.0145
Current minimum: -0.0520
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1289
Function value obtained: -0.0231
Current minimum: -0.0520
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0438
Current minimum: -0.0520
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1290
Function value obtained: -0.0302
Current minimum: -0.0520
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1280
Function value obtained: -0.0364
Current minimum: -0.0520
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1286
Function value obtained: -0.0339
Current minimum: -0.0520
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0446
Function value obtained: -0.0553
Current minimum: -0.0553
0.05529108577863005 0.1372329839434452
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1293
Function value obtained: -0.0565
Current minimum: -0.0565
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0781
Current minimum: -0.0781
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1289
Function value obtained: -0.1068
Current minimum: -0.1068
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1294
Function value obtained: -0.0704
Current minimum: -0.1068
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1293
Function value obtained: -0.0933
Current minimum: -0.1068
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1287
Function value obtained: -0.0814
Current minimum: -0.1068
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1286
Function value obtained: -0.0822
Current minimum: -0.1068
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1287
Function value obtained: -0.1053
Current minimum: -0.1068
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0251
Current minimum: -0.1068
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0638
Function value obtained: -0.1399
Current minimum: -0.1399
0.13991827125336548 0.13090342486223927
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0253
Current minimum: -0.0253
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0379
Current minimum: -0.0379
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1295
Function value obtained: -0.0319
Current minimum: -0.0379
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0218
Current minimum: -0.0379
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1300
Function value obtained: -0.0601
Current minimum: -0.0601
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0317
Current minimum: -0.0601
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1300
Function value obtained: -0.0160
Current minimum: -0.0601
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1301
Function value obtained: -0.0529
Current minimum: -0.0601
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1307
Function value obtained: -0.0180
Current minimum: -0.0601
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0251
Function value obtained: -0.0346
Current minimum: -0.0601
0.0600880245126167 0.14074648515328408
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1302
Function value obtained: -0.0055
Current minimum: -0.0055
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.0080
Current minimum: -0.0080
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0060
Current minimum: -0.0080
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1301
Function value obtained: -0.0092
Current minimum: -0.0092
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1302
Function value obtained: -0.0087
Current minimum: -0.0092
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0069
Current minimum: -0.0092
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1304
Function value obtained: -0.0074
Current minimum: -0.0092
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0040
Current minimum: -0.0092
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0071
Current minimum: -0.0092
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.2006
Function value obtained: -0.0058
Current minimum: -0.0092
0.009216099700544118 0.13363783624059977
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1304
Function value obtained: -0.0024
Current minimum: -0.0024
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1295
Function value obtained: -0.0025
Current minimum: -0.0025
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0034
Current minimum: -0.0034
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0033
Current minimum: -0.0034
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1288
Function value obtained: -0.0023
Current minimum: -0.0034
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1282
Function value obtained: -0.0032
Current minimum: -0.0034
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1288
Function value obtained: -0.0031
Current minimum: -0.0034
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1288
Function value obtained: -0.0032
Current minimum: -0.0034
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0038
Current minimum: -0.0038
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 8.9731
Function value obtained: -0.0033
Current minimum: -0.0038
0.0038282001660498214 0.13583374862848374
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1310
Function value obtained: -0.0575
Current minimum: -0.0575
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1313
Function value obtained: -0.0340
Current minimum: -0.0575
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1303
Function value obtained: -0.0478
Current minimum: -0.0575
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0386
Current minimum: -0.0575
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0243
Current minimum: -0.0575
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1281
Function value obtained: -0.0411
Current minimum: -0.0575
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1291
Function value obtained: -0.0297
Current minimum: -0.0575
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1290
Function value obtained: -0.0438
Current minimum: -0.0575
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0835
Current minimum: -0.0835
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.1324
Function value obtained: -0.0406
Current minimum: -0.0835
0.08351635865734426 0.1342367886584607
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1291
Function value obtained: -0.0135
Current minimum: -0.0135
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0358
Current minimum: -0.0358
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1289
Function value obtained: -0.0304
Current minimum: -0.0358
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1287
Function value obtained: -0.0152
Current minimum: -0.0358
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1303
Function value obtained: -0.0175
Current minimum: -0.0358
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0268
Current minimum: -0.0358
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1295
Function value obtained: -0.0145
Current minimum: -0.0358
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1290
Function value obtained: -0.0219
Current minimum: -0.0358
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1291
Function value obtained: -0.0163
Current minimum: -0.0358
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 8.9844
Function value obtained: -0.0136
Current minimum: -0.0358
0.0357559085264927 0.1331569775741017
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1302
Function value obtained: -0.0325
Current minimum: -0.0325
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1281
Function value obtained: -0.0456
Current minimum: -0.0456
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1287
Function value obtained: -0.0271
Current minimum: -0.0456
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1291
Function value obtained: -0.0228
Current minimum: -0.0456
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0300
Current minimum: -0.0456
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0202
Current minimum: -0.0456
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0180
Current minimum: -0.0456
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1291
Function value obtained: -0.0314
Current minimum: -0.0456
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1280
Function value obtained: -0.0242
Current minimum: -0.0456
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 8.9487
Function value obtained: -0.0215
Current minimum: -0.0456
0.045561458730289855 0.13377746540133
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0051
Current minimum: -0.0051
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1278
Function value obtained: -0.0154
Current minimum: -0.0154
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1286
Function value obtained: -0.0201
Current minimum: -0.0201
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0144
Current minimum: -0.0201
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0151
Current minimum: -0.0201
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1357
Function value obtained: -0.0122
Current minimum: -0.0201
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1286
Function value obtained: -0.0140
Current minimum: -0.0201
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1289
Function value obtained: -0.0161
Current minimum: -0.0201
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0170
Current minimum: -0.0201
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 8.8582
Function value obtained: -0.0127
Current minimum: -0.0201
0.02009586225796433 0.13401518300580045
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1296
Function value obtained: -0.0151
Current minimum: -0.0151
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0086
Current minimum: -0.0151
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0048
Current minimum: -0.0151
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.0163
Current minimum: -0.0163
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1301
Function value obtained: -0.0363
Current minimum: -0.0363
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.0399
Current minimum: -0.0399
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1310
Function value obtained: -0.0311
Current minimum: -0.0399
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0067
Current minimum: -0.0399
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0214
Current minimum: -0.0399
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0048
Function value obtained: -0.0109
Current minimum: -0.0399
0.03994664920271636 0.13801567781886576
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1309
Function value obtained: -0.0419
Current minimum: -0.0419
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0830
Current minimum: -0.0830
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1290
Function value obtained: -0.0352
Current minimum: -0.0830
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0544
Current minimum: -0.0830
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.1013
Current minimum: -0.1013
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1300
Function value obtained: -0.0821
Current minimum: -0.1013
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1291
Function value obtained: -0.0422
Current minimum: -0.1013
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1275
Function value obtained: -0.0891
Current minimum: -0.1013
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1293
Function value obtained: -0.0844
Current minimum: -0.1013
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0871
Function value obtained: -0.0750
Current minimum: -0.1013
0.10129053754775552 0.1339395016247999
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1301
Function value obtained: -0.0387
Current minimum: -0.0387
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1286
Function value obtained: -0.0200
Current minimum: -0.0387
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1289
Function value obtained: -0.0476
Current minimum: -0.0476
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1310
Function value obtained: -0.0168
Current minimum: -0.0476
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1292
Function value obtained: -0.0200
Current minimum: -0.0476
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1276
Function value obtained: -0.0161
Current minimum: -0.0476
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1290
Function value obtained: -0.0356
Current minimum: -0.0476
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1284
Function value obtained: -0.0148
Current minimum: -0.0476
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1288
Function value obtained: -0.0256
Current minimum: -0.0476
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.1391
Function value obtained: -0.0182
Current minimum: -0.0476
0.04762163940510956 0.13179970670617303
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1301
Function value obtained: -0.0449
Current minimum: -0.0449
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.0303
Current minimum: -0.0449
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0322
Current minimum: -0.0449
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1300
Function value obtained: -0.0210
Current minimum: -0.0449
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1294
Function value obtained: -0.0410
Current minimum: -0.0449
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1276
Function value obtained: -0.0434
Current minimum: -0.0449
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1286
Function value obtained: -0.0153
Current minimum: -0.0449
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1289
Function value obtained: -0.0269
Current minimum: -0.0449
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1274
Function value obtained: -0.0098
Current minimum: -0.0449
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.2060
Function value obtained: -0.0305
Current minimum: -0.0449
0.04493194137904809 0.13553804959298296
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1287
Function value obtained: -0.0098
Current minimum: -0.0098
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1288
Function value obtained: -0.0035
Current minimum: -0.0098
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1293
Function value obtained: -0.0052
Current minimum: -0.0098
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1294
Function value obtained: -0.0074
Current minimum: -0.0098
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1295
Function value obtained: -0.0090
Current minimum: -0.0098
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1295
Function value obtained: -0.0088
Current minimum: -0.0098
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1300
Function value obtained: -0.0128
Current minimum: -0.0128
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1293
Function value obtained: -0.0099
Current minimum: -0.0128
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1285
Function value obtained: -0.0105
Current minimum: -0.0128
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 8.9020
Function value obtained: -0.0109
Current minimum: -0.0128
0.012805432437280601 0.13844878946471006
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1303
Function value obtained: -0.0753
Current minimum: -0.0753
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0391
Current minimum: -0.0753
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0707
Current minimum: -0.0753
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.0746
Current minimum: -0.0753
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0346
Current minimum: -0.0753
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0498
Current minimum: -0.0753
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1309
Function value obtained: -0.0531
Current minimum: -0.0753
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1294
Function value obtained: -0.1218
Current minimum: -0.1218
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1284
Function value obtained: -0.0545
Current minimum: -0.1218
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0929
Function value obtained: -0.0348
Current minimum: -0.1218
0.12183529137074746 0.13394142447274704
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1330
Function value obtained: -0.0194
Current minimum: -0.0194
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1294
Function value obtained: -0.0151
Current minimum: -0.0194
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1302
Function value obtained: -0.0096
Current minimum: -0.0194
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1299
Function value obtained: -0.0113
Current minimum: -0.0194
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.0252
Current minimum: -0.0252
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1293
Function value obtained: -0.0077
Current minimum: -0.0252
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1276
Function value obtained: -0.0215
Current minimum: -0.0252
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1287
Function value obtained: -0.0089
Current minimum: -0.0252
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1288
Function value obtained: -0.0237
Current minimum: -0.0252
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0163
Function value obtained: -0.0109
Current minimum: -0.0252
0.025224988826540663 0.1335805421416589
Running BlackBox Minimization with Bayesian Optimization
Iteration No: 1 started. Evaluating function at random point.
Iteration No: 1 ended. Evaluation done at random point.
Time taken: 0.1306
Function value obtained: -0.0299
Current minimum: -0.0299
Iteration No: 2 started. Evaluating function at random point.
Iteration No: 2 ended. Evaluation done at random point.
Time taken: 0.1297
Function value obtained: -0.0242
Current minimum: -0.0299
Iteration No: 3 started. Evaluating function at random point.
Iteration No: 3 ended. Evaluation done at random point.
Time taken: 0.1298
Function value obtained: -0.0249
Current minimum: -0.0299
Iteration No: 4 started. Evaluating function at random point.
Iteration No: 4 ended. Evaluation done at random point.
Time taken: 0.1295
Function value obtained: -0.0280
Current minimum: -0.0299
Iteration No: 5 started. Evaluating function at random point.
Iteration No: 5 ended. Evaluation done at random point.
Time taken: 0.1307
Function value obtained: -0.0255
Current minimum: -0.0299
Iteration No: 6 started. Evaluating function at random point.
Iteration No: 6 ended. Evaluation done at random point.
Time taken: 0.1305
Function value obtained: -0.0308
Current minimum: -0.0308
Iteration No: 7 started. Evaluating function at random point.
Iteration No: 7 ended. Evaluation done at random point.
Time taken: 0.1302
Function value obtained: -0.0252
Current minimum: -0.0308
Iteration No: 8 started. Evaluating function at random point.
Iteration No: 8 ended. Evaluation done at random point.
Time taken: 0.1304
Function value obtained: -0.0236
Current minimum: -0.0308
Iteration No: 9 started. Evaluating function at random point.
Iteration No: 9 ended. Evaluation done at random point.
Time taken: 0.1302
Function value obtained: -0.0225
Current minimum: -0.0308
Iteration No: 10 started. Evaluating function at random point.
Iteration No: 10 ended. Evaluation done at random point.
Time taken: 9.0532
Function value obtained: -0.0268
Current minimum: -0.0308
0.030841481944845366 0.13619234587263598
Missed points: 0/20
> /home/lgpu0014/FACT/SENN/scripts/main_mnist.py(384)main()
-> Stability_dict = {'lips': lips}
(Pdb) 
/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.
  warnings.warn(message, FutureWarning)
/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.
  warnings.warn(message, FutureWarning)
/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.
  warnings.warn(msg, category=FutureWarning)
/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.
  warnings.warn(message, FutureWarning)
Traceback (most recent call last):
  File "scripts/main_mnist.py", line 404, in <module>
    main()
  File "scripts/main_mnist.py", line 384, in main
    Stability_dict = {'lips': lips}
  File "scripts/main_mnist.py", line 384, in main
    Stability_dict = {'lips': lips}
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/bdb.py", line 51, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/bdb.py", line 70, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
srun: error: r30n6: task 0: Exited with exit code 1
srun: Job 4386953 step creation temporarily disabled, retrying
srun: Step created for job 4386953

Parameters:
	BATCH_SIZE=128
	CONCEPT_DIM=1
	CUDA=True
	DATASET=pathology
	DATASETS=['heart', 'ionosphere', 'breast-cancer', 'wine', 'heart', 'glass', 'diabetes', 'yeast', 'leukemia', 'abalone']
	DEBUG=False
	DROPOUT=0.1
	EMBEDDING=pathology
	EPOCHS=10
	H_SPARSITY=0.0001
	H_TYPE=cnn
	LIP_CALLS=10
	LIP_EPS=0.01
	LIP_POINTS=100
	LISA=1
	LOAD_MODEL=False
	LOG_PATH=log
	LR=0.001
	MODEL_PATH=models
	NCLASSES=2
	NCONCEPTS=15
	NOBIAS=False
	NUM_GPUS=1
	NUM_WORKERS=4
	OBJECTIVE=cross_entropy
	OPT=adam
	OPTIM=gp
	POSITIVE_THETA=False
	PRINT_FREQ=10
	RESULTS_PATH=out
	SEED=2018
	SUMMARY_PATH=results/summary.csv
	TEST=False
	THETA_ARCH=simple
	THETA_DIM=-1
	THETA_REG_LAMBDA=0.01
	THETA_REG_TYPE=grad3
	TRAIN=True
	WEIGHT_DECAY=0.001
Model path out lisa_output/models/mnist/grad3_Hcnn_Thsimple_Cpts15_Reg1e-02_Sp0.0001_LR0.001
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2aab9caf9dd8>>
Traceback (most recent call last):
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/queues.py", line 337, in get
    return _ForkingPickler.loads(res)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 493, in Client
    answer_challenge(c, authkey)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 732, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError: 
Traceback (most recent call last):
  File "scripts/main_mnist.py", line 404, in <module>
    main()
  File "scripts/main_mnist.py", line 257, in main
    trainer.train(train_loader, valid_loader, epochs = args.epochs, save_path = model_path)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 180, in train
    self.train_epoch(epoch, train_loader)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 248, in train_epoch
    outputs, loss, loss_dict = self.train_batch(inputs, targets)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 575, in train_batch
    pred = self.model(inputs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/models.py", line 221, in forward
    h_x, x_tilde = self.conceptizer(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 102, in forward
    encoded = self.encode(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 202, in encode
    p       = F.relu(F.max_pool2d(self.conv1(x), 2))
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'
srun: error: r31n3: task 0: Exited with exit code 1
srun: Job 4386967 step creation temporarily disabled, retrying
srun: Step created for job 4386967

Parameters:
	BATCH_SIZE=128
	CONCEPT_DIM=1
	CUDA=True
	DATASET=pathology
	DATASETS=['heart', 'ionosphere', 'breast-cancer', 'wine', 'heart', 'glass', 'diabetes', 'yeast', 'leukemia', 'abalone']
	DEBUG=False
	DROPOUT=0.1
	EMBEDDING=pathology
	EPOCHS=10
	H_SPARSITY=0.0001
	H_TYPE=cnn
	LIP_CALLS=10
	LIP_EPS=0.01
	LIP_POINTS=100
	LISA=1
	LOAD_MODEL=False
	LOG_PATH=log
	LR=0.001
	MODEL_PATH=models
	NCLASSES=2
	NCONCEPTS=15
	NOBIAS=False
	NUM_GPUS=1
	NUM_WORKERS=4
	OBJECTIVE=cross_entropy
	OPT=adam
	OPTIM=gp
	POSITIVE_THETA=False
	PRINT_FREQ=10
	RESULTS_PATH=out
	SEED=2018
	SUMMARY_PATH=results/summary.csv
	TEST=False
	THETA_ARCH=simple
	THETA_DIM=-1
	THETA_REG_LAMBDA=0.01
	THETA_REG_TYPE=grad3
	TRAIN=True
	WEIGHT_DECAY=0.001
Model path out lisa_output/models/mnist/grad3_Hcnn_Thsimple_Cpts15_Reg1e-02_Sp0.0001_LR0.001
Traceback (most recent call last):
  File "scripts/main_mnist.py", line 404, in <module>
    main()
  File "scripts/main_mnist.py", line 257, in main
    trainer.train(train_loader, valid_loader, epochs = args.epochs, save_path = model_path)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 180, in train
    self.train_epoch(epoch, train_loader)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 248, in train_epoch
    outputs, loss, loss_dict = self.train_batch(inputs, targets)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 575, in train_batch
    pred = self.model(inputs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/models.py", line 221, in forward
    h_x, x_tilde = self.conceptizer(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 102, in forward
    encoded = self.encode(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 202, in encode
    p       = F.relu(F.max_pool2d(self.conv1(x), 2))
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'
srun: error: r31n3: task 0: Exited with exit code 1
srun: Job 4386986 step creation temporarily disabled, retrying
srun: Step created for job 4386986

Parameters:
	BATCH_SIZE=128
	CONCEPT_DIM=1
	CUDA=True
	DATASET=pathology
	DATASETS=['heart', 'ionosphere', 'breast-cancer', 'wine', 'heart', 'glass', 'diabetes', 'yeast', 'leukemia', 'abalone']
	DEBUG=False
	DROPOUT=0.1
	EMBEDDING=pathology
	EPOCHS=10
	H_SPARSITY=0.0001
	H_TYPE=cnn
	LIP_CALLS=10
	LIP_EPS=0.01
	LIP_POINTS=100
	LISA=1
	LOAD_MODEL=False
	LOG_PATH=log
	LR=0.001
	MODEL_PATH=models
	NCLASSES=2
	NCONCEPTS=15
	NOBIAS=False
	NUM_GPUS=1
	NUM_WORKERS=4
	OBJECTIVE=cross_entropy
	OPT=adam
	OPTIM=gp
	POSITIVE_THETA=False
	PRINT_FREQ=10
	RESULTS_PATH=out
	SEED=2018
	SUMMARY_PATH=results/summary.csv
	TEST=False
	THETA_ARCH=simple
	THETA_DIM=-1
	THETA_REG_LAMBDA=0.01
	THETA_REG_TYPE=grad3
	TRAIN=True
	WEIGHT_DECAY=0.001
Model path out lisa_output/models/mnist/grad3_Hcnn_Thsimple_Cpts15_Reg1e-02_Sp0.0001_LR0.001
Traceback (most recent call last):
  File "scripts/main_mnist.py", line 404, in <module>
    main()
  File "scripts/main_mnist.py", line 257, in main
    trainer.train(train_loader, valid_loader, epochs = args.epochs, save_path = model_path)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 180, in train
    self.train_epoch(epoch, train_loader)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 248, in train_epoch
    outputs, loss, loss_dict = self.train_batch(inputs, targets)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 575, in train_batch
    pred = self.model(inputs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/models.py", line 221, in forward
    h_x, x_tilde = self.conceptizer(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 102, in forward
    encoded = self.encode(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 202, in encode
    p       = F.relu(F.max_pool2d(self.conv1(x), 2))
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'
srun: error: r31n3: task 0: Exited with exit code 1
srun: Job 4386990 step creation temporarily disabled, retrying
srun: Step created for job 4386990

Parameters:
	BATCH_SIZE=128
	CONCEPT_DIM=1
	CUDA=True
	DATASET=pathology
	DATASETS=['heart', 'ionosphere', 'breast-cancer', 'wine', 'heart', 'glass', 'diabetes', 'yeast', 'leukemia', 'abalone']
	DEBUG=False
	DROPOUT=0.1
	EMBEDDING=pathology
	EPOCHS=10
	H_SPARSITY=0.0001
	H_TYPE=cnn
	LIP_CALLS=10
	LIP_EPS=0.01
	LIP_POINTS=100
	LISA=1
	LOAD_MODEL=False
	LOG_PATH=log
	LR=0.001
	MODEL_PATH=models
	NCLASSES=2
	NCONCEPTS=15
	NOBIAS=False
	NUM_GPUS=1
	NUM_WORKERS=4
	OBJECTIVE=cross_entropy
	OPT=adam
	OPTIM=gp
	POSITIVE_THETA=False
	PRINT_FREQ=10
	RESULTS_PATH=out
	SEED=2018
	SUMMARY_PATH=results/summary.csv
	TEST=False
	THETA_ARCH=simple
	THETA_DIM=-1
	THETA_REG_LAMBDA=0.01
	THETA_REG_TYPE=grad3
	TRAIN=True
	WEIGHT_DECAY=0.001
Model path out lisa_output/models/mnist/grad3_Hcnn_Thsimple_Cpts15_Reg1e-02_Sp0.0001_LR0.001
Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2baa6c1e9da0>>
Traceback (most recent call last):
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 399, in __del__
    self._shutdown_workers()
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 378, in _shutdown_workers
    self.worker_result_queue.get()
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/queues.py", line 337, in get
    return _ForkingPickler.loads(res)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/multiprocessing/reductions.py", line 151, in rebuild_storage_fd
    fd = df.detach()
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 493, in Client
    answer_challenge(c, authkey)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 732, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError: 
Traceback (most recent call last):
  File "scripts/main_mnist.py", line 404, in <module>
    main()
  File "scripts/main_mnist.py", line 257, in main
    trainer.train(train_loader, valid_loader, epochs = args.epochs, save_path = model_path)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 180, in train
    self.train_epoch(epoch, train_loader)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 248, in train_epoch
    outputs, loss, loss_dict = self.train_batch(inputs, targets)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/trainers.py", line 575, in train_batch
    pred = self.model(inputs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/models.py", line 221, in forward
    h_x, x_tilde = self.conceptizer(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 102, in forward
    encoded = self.encode(x)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/SENN/conceptizers.py", line 202, in encode
    p       = F.relu(F.max_pool2d(self.conv1(x), 2))
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/module.py", line 477, in __call__
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0014/.conda/envs/FACT/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 301, in forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'
srun: error: r31n3: task 0: Exited with exit code 1
